{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84c637d2-b88a-473b-b909-b2bd633022fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from tabasco.chem.convert import MoleculeConverter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0ea01cf-f732-4601-889b-683475a7514f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MolecularEncoder(nn.Module):\n",
    "    \"\"\"Base class for frozen molecular encoders used in REPA.\"\"\"\n",
    "\n",
    "    def forward(self, coords, atomics, padding_mask):\n",
    "        \"\"\"Extract representations from molecules.\n",
    "\n",
    "        Args:\n",
    "            coords: [B, N, 3] - atomic coordinates\n",
    "            atomics: [B, N, atom_dim] - one-hot atom types\n",
    "            padding_mask: [B, N] - True for padding\n",
    "\n",
    "        Returns:\n",
    "            repr: [B, N, encoder_dim] - molecular representations\n",
    "        \"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46992ca5-7fce-4e7d-b359-004f2b2d015e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChemPropEncoder(MolecularEncoder):\n",
    "    \"\"\"Encoder using ChemProp's message passing neural network.\n",
    "\n",
    "    ChemProp is a 2D graph-based encoder that operates on molecular topology.\n",
    "    It does NOT use 3D coordinates - only atom types and bond connectivity.\n",
    "\n",
    "    This encoder converts Tabasco's (coords, atomics) format to RDKit molecules,\n",
    "    then uses ChemProp's BondMessagePassing to get atom-level embeddings.\n",
    "\n",
    "    Note: Since ChemProp is graph-based (not 3D), the coords are only used\n",
    "    to infer bond connectivity via RDKit's DetermineBonds algorithm.\n",
    "\n",
    "    For REPA to work effectively, we use pretrained weights (e.g., CheMeleon)\n",
    "    so the encoder provides meaningful molecular representations as alignment targets.\n",
    "    https://github.com/chemprop/chemprop/blob/main/examples/chemeleon_foundation_finetuning.ipynb\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_dim: int = 300,\n",
    "        depth: int = 3,\n",
    "        dropout: float = 0.0,\n",
    "        pretrained: str = \"chemeleon\",\n",
    "    ):\n",
    "        \"\"\"Initialize ChemProp encoder.\n",
    "\n",
    "        Args:\n",
    "            encoder_dim: Output dimension of message passing (ignored if using pretrained)\n",
    "            depth: Number of message passing iterations (ignored if using pretrained)\n",
    "            dropout: Dropout probability (ignored if using pretrained)\n",
    "            pretrained: One of:\n",
    "                - \"chemeleon\": Load CheMeleon foundation model (recommended, auto-downloads)\n",
    "                - \"none\" or None: Random initialization (not recommended for REPA)\n",
    "                - Path to a .pt file: Load custom pretrained weights\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        from chemprop.nn import BondMessagePassing\n",
    "        from chemprop.featurizers import SimpleMoleculeMolGraphFeaturizer\n",
    "\n",
    "        self.featurizer = SimpleMoleculeMolGraphFeaturizer()\n",
    "\n",
    "        if pretrained is None or pretrained == \"none\":\n",
    "            # Random initialization (not recommended for REPA)\n",
    "            import warnings\n",
    "\n",
    "            warnings.warn(\n",
    "                \"ChemPropEncoder initialized with random weights. \"\n",
    "                \"For REPA to be effective, use pretrained='chemeleon' to load \"\n",
    "                \"the CheMeleon foundation model with meaningful molecular representations.\"\n",
    "            )\n",
    "            self.encoder_dim = encoder_dim\n",
    "            self.message_passing = BondMessagePassing(\n",
    "                d_h=encoder_dim,\n",
    "                depth=depth,\n",
    "                dropout=dropout,\n",
    "            )\n",
    "        elif pretrained == \"chemeleon\":\n",
    "            # Load CheMeleon foundation model\n",
    "            chemeleon_weights = self._load_chemeleon()\n",
    "            self.message_passing = BondMessagePassing(\n",
    "                **chemeleon_weights[\"hyper_parameters\"]\n",
    "            )\n",
    "            self.message_passing.load_state_dict(chemeleon_weights[\"state_dict\"])\n",
    "            self.encoder_dim = chemeleon_weights[\"hyper_parameters\"][\"d_h\"]\n",
    "        else:\n",
    "            # Load from custom path\n",
    "            weights = torch.load(pretrained, map_location=\"cpu\", weights_only=True)\n",
    "            if \"hyper_parameters\" in weights:\n",
    "                # ChemProp format\n",
    "                self.message_passing = BondMessagePassing(**weights[\"hyper_parameters\"])\n",
    "                self.message_passing.load_state_dict(weights[\"state_dict\"])\n",
    "                self.encoder_dim = weights[\"hyper_parameters\"][\"d_h\"]\n",
    "            else:\n",
    "                # Raw state dict\n",
    "                self.encoder_dim = encoder_dim\n",
    "                self.message_passing = BondMessagePassing(\n",
    "                    d_h=encoder_dim,\n",
    "                    depth=depth,\n",
    "                    dropout=dropout,\n",
    "                )\n",
    "                self.message_passing.load_state_dict(weights)\n",
    "\n",
    "        # Molecule converter for coords/atomics -> RDKit mol\n",
    "        self.converter = MoleculeConverter()\n",
    "\n",
    "    def _load_chemeleon(self) -> dict:\n",
    "        \"\"\"Download and load CheMeleon foundation model weights.\n",
    "\n",
    "        CheMeleon is pretrained on 1M molecules from PubChem and provides\n",
    "        meaningful molecular representations for REPA alignment.\n",
    "\n",
    "        Returns:\n",
    "            dict with \"hyper_parameters\" and \"state_dict\" keys\n",
    "        \"\"\"\n",
    "        from pathlib import Path\n",
    "        from urllib.request import urlretrieve\n",
    "        import logging\n",
    "\n",
    "        logger = logging.getLogger(__name__)\n",
    "\n",
    "        ckpt_dir = Path.home() / \".chemprop\"\n",
    "        ckpt_dir.mkdir(exist_ok=True)\n",
    "        model_path = ckpt_dir / \"chemeleon_mp.pt\"\n",
    "\n",
    "        if not model_path.exists():\n",
    "            logger.info(\n",
    "                f\"Downloading CheMeleon foundation model from Zenodo to {model_path}\"\n",
    "            )\n",
    "            urlretrieve(\n",
    "                \"https://zenodo.org/records/15460715/files/chemeleon_mp.pt\", model_path\n",
    "            )\n",
    "        else:\n",
    "            logger.info(f\"Loading cached CheMeleon from {model_path}\")\n",
    "\n",
    "        return torch.load(model_path, map_location=\"cpu\", weights_only=True)\n",
    "\n",
    "    def forward(self, coords, atomics, padding_mask):\n",
    "        \"\"\"Extract ChemProp atom embeddings.\n",
    "\n",
    "        Args:\n",
    "            coords: [B, N, 3] - atomic coordinates (used for bond inference)\n",
    "            atomics: [B, N, atom_dim] - one-hot atom types\n",
    "            padding_mask: [B, N] - True for padding positions\n",
    "\n",
    "        Returns:\n",
    "            repr: [B, N, encoder_dim] - atom-level representations\n",
    "        \"\"\"\n",
    "        from tensordict import TensorDict\n",
    "        from chemprop.data import BatchMolGraph\n",
    "\n",
    "        B, N, _ = coords.shape\n",
    "        device = coords.device\n",
    "\n",
    "        # Convert each molecule in batch to RDKit mol, then to ChemProp MolGraph\n",
    "        molgraphs = []\n",
    "        atom_counts = []  # Track atoms per molecule for later padding\n",
    "\n",
    "        for i in range(B):\n",
    "            # Create TensorDict for this molecule\n",
    "            mol_td = TensorDict(\n",
    "                {\n",
    "                    \"coords\": coords[i],\n",
    "                    \"atomics\": atomics[i],\n",
    "                    \"padding_mask\": padding_mask[i],\n",
    "                }\n",
    "            )\n",
    "\n",
    "            # Convert to RDKit mol (this handles bond inference)\n",
    "            try:\n",
    "                mol = self.converter.from_tensor(\n",
    "                    mol_td,\n",
    "                    rescale_coords=True,\n",
    "                    sanitize=False,  # Don't sanitize to avoid failures\n",
    "                    use_openbabel=False,  # Use simpler bond inference\n",
    "                )\n",
    "                if mol is not None:\n",
    "                    mg = self.featurizer(mol)\n",
    "                    molgraphs.append(mg)\n",
    "                    atom_counts.append(mol.GetNumAtoms())\n",
    "                else:\n",
    "                    # Failed conversion - use placeholder\n",
    "                    molgraphs.append(None)\n",
    "                    atom_counts.append(0)\n",
    "            except Exception:\n",
    "                molgraphs.append(None)\n",
    "                atom_counts.append(0)\n",
    "\n",
    "        # Filter out failed conversions\n",
    "        valid_mgs = [mg for mg in molgraphs if mg is not None]\n",
    "\n",
    "        if len(valid_mgs) == 0:\n",
    "            # All molecules failed - return zeros\n",
    "            return torch.zeros(B, N, self.encoder_dim, device=device)\n",
    "\n",
    "        # Create batch and run message passing\n",
    "        bmg = BatchMolGraph(valid_mgs)\n",
    "        bmg.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            atom_embeddings = self.message_passing(bmg)  # [total_atoms, encoder_dim]\n",
    "\n",
    "        # Reconstruct batched output with padding\n",
    "        output = torch.zeros(B, N, self.encoder_dim, device=device)\n",
    "\n",
    "        # Map embeddings back to batch positions\n",
    "        valid_idx = 0\n",
    "        emb_offset = 0\n",
    "        for i in range(B):\n",
    "            if molgraphs[i] is not None:\n",
    "                n_atoms = atom_counts[valid_idx]\n",
    "                # Copy atom embeddings to output, respecting padding\n",
    "                output[i, :n_atoms] = atom_embeddings[emb_offset : emb_offset + n_atoms]\n",
    "                emb_offset += n_atoms\n",
    "                valid_idx += 1\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc63ff3-0a5e-4577-9115-d95417263527",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
