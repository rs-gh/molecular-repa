{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug REPA Training\n",
    "\n",
    "This notebook steps through forward/backward passes with real QM9 data to verify:\n",
    "- Correct parameter training (net + projector trainable, encoder frozen)\n",
    "- Gradient flow\n",
    "- REPA loss computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from tensordict import TensorDict\n",
    "\n",
    "# Change to project root\n",
    "os.chdir(\"/Users/shreyas/git/molecular-repa/src/tabasco\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabasco.chem.constants import ATOM_NAMES\n",
    "\n",
    "\n",
    "def decode_atoms(atomics_tensor):\n",
    "    \"\"\"Convert one-hot encoded atomics to atom symbols.\n",
    "\n",
    "    Args:\n",
    "        atomics_tensor: Tensor of shape [..., atom_dim] with one-hot encodings\n",
    "\n",
    "    Returns:\n",
    "        List of atom symbols (e.g., ['C', 'N', 'O', ...])\n",
    "    \"\"\"\n",
    "    indices = atomics_tensor.argmax(dim=-1)\n",
    "    if indices.dim() == 0:\n",
    "        return ATOM_NAMES[indices.item()]\n",
    "    return [ATOM_NAMES[i] for i in indices.tolist()]\n",
    "\n",
    "\n",
    "def decode_molecule(atomics_tensor, padding_mask=None):\n",
    "    \"\"\"Decode a molecule's atoms, optionally filtering padding.\n",
    "\n",
    "    Args:\n",
    "        atomics_tensor: Tensor of shape [num_atoms, atom_dim]\n",
    "        padding_mask: Optional bool tensor, True = padded atom\n",
    "\n",
    "    Returns:\n",
    "        List of atom symbols for non-padded atoms\n",
    "    \"\"\"\n",
    "    atoms = decode_atoms(atomics_tensor)\n",
    "    if padding_mask is not None:\n",
    "        atoms = [a for a, pad in zip(atoms, padding_mask.tolist()) if not pad]\n",
    "    return atoms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load QM9 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch keys: _StringKeys(dict_keys(['coords', 'atomics', 'padding_mask', 'index']))\n",
      "coords shape: torch.Size([4, 9, 3])\n",
      "atomics shape: torch.Size([4, 9, 9])\n",
      "padding_mask shape: torch.Size([4, 9])\n"
     ]
    }
   ],
   "source": [
    "from tabasco.data.lmdb_datamodule import LmdbDataModule\n",
    "\n",
    "dm = LmdbDataModule(\n",
    "    data_dir=\"data/processed_qm9_train.pt\",\n",
    "    val_data_dir=\"data/processed_qm9_val.pt\",\n",
    "    lmdb_dir=\"data/lmdb_qm9\",\n",
    "    batch_size=4,\n",
    "    num_workers=0,\n",
    ")\n",
    "dm.prepare_data()\n",
    "dm.setup()\n",
    "batch = next(iter(dm.train_dataloader()))\n",
    "\n",
    "print(f\"Batch keys: {batch.keys()}\")\n",
    "print(f\"coords shape: {batch['coords'].shape}\")\n",
    "print(f\"atomics shape: {batch['atomics'].shape}\")\n",
    "print(f\"padding_mask shape: {batch['padding_mask'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example molecule:\n",
      "Molecule coordinates: tensor([[ 0.3589, -0.6821, -1.3073],\n",
      "        [ 0.1212, -0.4510, -0.8060],\n",
      "        [-0.1592, -0.1688, -0.1945],\n",
      "        [-0.1550,  0.5973, -0.1987],\n",
      "        [-0.5828,  0.8061,  0.3204],\n",
      "        [-0.5385,  1.4965,  0.4524],\n",
      "        [ 0.2366, -0.4326,  0.4195],\n",
      "        [ 0.0558, -1.0599,  0.6157],\n",
      "        [ 0.6629, -0.1056,  0.6984]])\n",
      "Molecule atomics: tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.]])\n",
      "Molecule padding_mask: tensor([False, False, False, False, False, False, False, False, False])\n"
     ]
    }
   ],
   "source": [
    "print(\"Example molecule:\")\n",
    "print(f\"Molecule coordinates: {batch['coords'][0]}\")\n",
    "print(f\"Molecule atomics: {batch['atomics'][0]}\")\n",
    "print(f\"Molecule padding_mask: {batch['padding_mask'][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Molecule 0: CCCCOCCNO (9 atoms)\n",
      "Molecule 1: CNCNNCCCN (9 atoms)\n",
      "Molecule 2: CCNCCOCCN (9 atoms)\n",
      "Molecule 3: CCCCCCCOC (9 atoms)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(batch[\"atomics\"])):\n",
    "    atoms = decode_molecule(batch[\"atomics\"][i], batch[\"padding_mask\"][i])\n",
    "    print(f\"Molecule {i}: {''.join(atoms)} ({len(atoms)} atoms)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 96334\n",
      "Val dataset size: 19775\n",
      "Train batches: 24084\n",
      "Val batches: 4944\n",
      "Batch size: 4\n",
      "Train molecules (approx): 96336\n"
     ]
    }
   ],
   "source": [
    "# Dataset sizes (number of molecules)\n",
    "print(f\"Train dataset size: {len(dm.train_dataset)}\")\n",
    "print(f\"Val dataset size: {len(dm.val_dataset)}\")\n",
    "\n",
    "# Dataloader info\n",
    "train_loader = dm.train_dataloader()\n",
    "val_loader = dm.val_dataloader()\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")\n",
    "print(f\"Batch size: {dm.batch_size}\")\n",
    "\n",
    "# Total molecules = batches * batch_size (approximately, last batch may be smaller)\n",
    "print(f\"Train molecules (approx): {len(train_loader) * dm.batch_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num atoms histogram: {1: 2, 2: 4, 3: 6, 4: 21, 5: 91, 6: 462, 7: 2298, 8: 13101, 9: 80349}\n",
      "Max atoms: 9\n"
     ]
    }
   ],
   "source": [
    "# See the full distribution from the dataset\n",
    "stats = dm.train_dataset.get_stats()\n",
    "print(f\"Num atoms histogram: {stats['num_atoms_histogram']}\")\n",
    "print(f\"Max atoms: {stats['max_num_atoms']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Baseline Model (no REPA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "atom_dim from data: 9\n",
      "Implementation: reimplemented\n"
     ]
    }
   ],
   "source": [
    "from tabasco.models.flow_model import FlowMatchingModel\n",
    "from tabasco.models.components.transformer_module import TransformerModule\n",
    "from tabasco.flow.interpolate import CenteredMetricInterpolant, DiscreteInterpolant\n",
    "\n",
    "atom_dim = batch[\"atomics\"].shape[-1]\n",
    "print(f\"atom_dim from data: {atom_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Implementation: reimplemented\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "net_baseline = TransformerModule(\n",
    "    hidden_dim=128,\n",
    "    num_layers=16,\n",
    "    num_heads=8,\n",
    "    atom_dim=atom_dim,\n",
    "    spatial_dim=3,\n",
    "    implementation=\"reimplemented\",\n",
    "    activation=\"SiLU\",\n",
    "    cross_attention=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline model created\n"
     ]
    }
   ],
   "source": [
    "baseline_model = FlowMatchingModel(\n",
    "    net=net_baseline,\n",
    "    coords_interpolant=CenteredMetricInterpolant(\n",
    "        key=\"coords\", key_pad_mask=\"padding_mask\"\n",
    "    ),\n",
    "    atomics_interpolant=DiscreteInterpolant(key=\"atomics\", key_pad_mask=\"padding_mask\"),\n",
    "    repa_loss=None,\n",
    ")\n",
    "\n",
    "print(\"Baseline model created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create REPA Model (with ChemProp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "net_repa = copy.deepcopy(net_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder dim: 2048\n",
      "REPA model created\n"
     ]
    }
   ],
   "source": [
    "from tabasco.models.components.encoders import ChemPropEncoder, Projector\n",
    "from tabasco.models.components.losses import REPALoss\n",
    "\n",
    "encoder = ChemPropEncoder(pretrained=\"chemeleon\")\n",
    "print(f\"Encoder dim: {encoder.encoder_dim}\")\n",
    "\n",
    "projector = Projector(hidden_dim=128, encoder_dim=encoder.encoder_dim)\n",
    "repa_loss = REPALoss(encoder=encoder, projector=projector, lambda_repa=0.5)\n",
    "\n",
    "repa_model = FlowMatchingModel(\n",
    "    net=net_repa,\n",
    "    coords_interpolant=CenteredMetricInterpolant(\n",
    "        key=\"coords\", key_pad_mask=\"padding_mask\"\n",
    "    ),\n",
    "    atomics_interpolant=DiscreteInterpolant(key=\"atomics\", key_pad_mask=\"padding_mask\"),\n",
    "    repa_loss=repa_loss,\n",
    ")\n",
    "\n",
    "print(\"REPA model created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Verify Parameter States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def check_params(model, name):\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "\n",
    "    # Categorize params\n",
    "    categories = {\n",
    "        \"net\": {\"trainable\": 0, \"frozen\": 0, \"params\": []},\n",
    "        \"projector\": {\"trainable\": 0, \"frozen\": 0, \"params\": []},\n",
    "        \"encoder\": {\"trainable\": 0, \"frozen\": 0, \"params\": []},\n",
    "        \"other\": {\"trainable\": 0, \"frozen\": 0, \"params\": []},\n",
    "    }\n",
    "\n",
    "    for n, p in model.named_parameters():\n",
    "        # Determine category\n",
    "        if n.startswith(\"net.\"):\n",
    "            cat = \"net\"\n",
    "        elif \"projector\" in n:\n",
    "            cat = \"projector\"\n",
    "        elif \"encoder\" in n:\n",
    "            cat = \"encoder\"\n",
    "        else:\n",
    "            cat = \"other\"\n",
    "\n",
    "        # Count params\n",
    "        if p.requires_grad:\n",
    "            categories[cat][\"trainable\"] += p.numel()\n",
    "        else:\n",
    "            categories[cat][\"frozen\"] += p.numel()\n",
    "        categories[cat][\"params\"].append((n, p.requires_grad, tuple(p.shape)))\n",
    "\n",
    "    # Print by category\n",
    "    for cat, data in categories.items():\n",
    "        total = data[\"trainable\"] + data[\"frozen\"]\n",
    "        if total == 0:\n",
    "            continue\n",
    "        print(f\"\\n--- {cat.upper()} ---\")\n",
    "        for n, req_grad, shape in data[\"params\"]:\n",
    "            status = \"TRAINABLE\" if req_grad else \"FROZEN\"\n",
    "            print(f\"  {n}: {status}, shape={shape}\")\n",
    "        print(f\"  Trainable: {data['trainable']:,}\")\n",
    "        print(f\"  Frozen: {data['frozen']:,}\")\n",
    "\n",
    "    # Summary\n",
    "    total_trainable = sum(d[\"trainable\"] for d in categories.values())\n",
    "    total_frozen = sum(d[\"frozen\"] for d in categories.values())\n",
    "    print(\"\\n--- SUMMARY ---\")\n",
    "    print(f\"{'Component':<12} {'Trainable':>12} {'Frozen':>12} {'Total':>12}\")\n",
    "    print(\"-\" * 50)\n",
    "    for cat, data in categories.items():\n",
    "        total = data[\"trainable\"] + data[\"frozen\"]\n",
    "        if total > 0:\n",
    "            print(\n",
    "                f\"{cat:<12} {data['trainable']:>12,} {data['frozen']:>12,} {total:>12,}\"\n",
    "            )\n",
    "    print(\"-\" * 50)\n",
    "    print(\n",
    "        f\"{'TOTAL':<12} {total_trainable:>12,} {total_frozen:>12,} {total_trainable + total_frozen:>12,}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Baseline ===\n",
      "\n",
      "--- NET ---\n",
      "  net.linear_embed.weight: TRAINABLE, shape=(128, 3)\n",
      "  net.atom_type_embed.weight: TRAINABLE, shape=(9, 128)\n",
      "  net.transformer.layers.0.attn_block.norm.weight: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.0.attn_block.norm.bias: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.0.attn_block.attention.mha.in_proj_weight: TRAINABLE, shape=(384, 128)\n",
      "  net.transformer.layers.0.attn_block.attention.mha.in_proj_bias: TRAINABLE, shape=(384,)\n",
      "  net.transformer.layers.0.attn_block.attention.mha.out_proj.weight: TRAINABLE, shape=(128, 128)\n",
      "  net.transformer.layers.0.attn_block.attention.mha.out_proj.bias: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.0.ff_block.w1.weight: TRAINABLE, shape=(512, 128)\n",
      "  net.transformer.layers.0.ff_block.w3.weight: TRAINABLE, shape=(128, 512)\n",
      "  net.transformer.layers.0.ff_block.norm.weight: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.0.ff_block.norm.bias: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.1.attn_block.norm.weight: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.1.attn_block.norm.bias: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.1.attn_block.attention.mha.in_proj_weight: TRAINABLE, shape=(384, 128)\n",
      "  net.transformer.layers.1.attn_block.attention.mha.in_proj_bias: TRAINABLE, shape=(384,)\n",
      "  net.transformer.layers.1.attn_block.attention.mha.out_proj.weight: TRAINABLE, shape=(128, 128)\n",
      "  net.transformer.layers.1.attn_block.attention.mha.out_proj.bias: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.1.ff_block.w1.weight: TRAINABLE, shape=(512, 128)\n",
      "  net.transformer.layers.1.ff_block.w3.weight: TRAINABLE, shape=(128, 512)\n",
      "  net.transformer.layers.1.ff_block.norm.weight: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.1.ff_block.norm.bias: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.2.attn_block.norm.weight: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.2.attn_block.norm.bias: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.2.attn_block.attention.mha.in_proj_weight: TRAINABLE, shape=(384, 128)\n",
      "  net.transformer.layers.2.attn_block.attention.mha.in_proj_bias: TRAINABLE, shape=(384,)\n",
      "  net.transformer.layers.2.attn_block.attention.mha.out_proj.weight: TRAINABLE, shape=(128, 128)\n",
      "  net.transformer.layers.2.attn_block.attention.mha.out_proj.bias: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.2.ff_block.w1.weight: TRAINABLE, shape=(512, 128)\n",
      "  net.transformer.layers.2.ff_block.w3.weight: TRAINABLE, shape=(128, 512)\n",
      "  net.transformer.layers.2.ff_block.norm.weight: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.2.ff_block.norm.bias: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.3.attn_block.norm.weight: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.3.attn_block.norm.bias: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.3.attn_block.attention.mha.in_proj_weight: TRAINABLE, shape=(384, 128)\n",
      "  net.transformer.layers.3.attn_block.attention.mha.in_proj_bias: TRAINABLE, shape=(384,)\n",
      "  net.transformer.layers.3.attn_block.attention.mha.out_proj.weight: TRAINABLE, shape=(128, 128)\n",
      "  net.transformer.layers.3.attn_block.attention.mha.out_proj.bias: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.3.ff_block.w1.weight: TRAINABLE, shape=(512, 128)\n",
      "  net.transformer.layers.3.ff_block.w3.weight: TRAINABLE, shape=(128, 512)\n",
      "  net.transformer.layers.3.ff_block.norm.weight: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.3.ff_block.norm.bias: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.4.attn_block.norm.weight: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.4.attn_block.norm.bias: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.4.attn_block.attention.mha.in_proj_weight: TRAINABLE, shape=(384, 128)\n",
      "  net.transformer.layers.4.attn_block.attention.mha.in_proj_bias: TRAINABLE, shape=(384,)\n",
      "  net.transformer.layers.4.attn_block.attention.mha.out_proj.weight: TRAINABLE, shape=(128, 128)\n",
      "  net.transformer.layers.4.attn_block.attention.mha.out_proj.bias: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.4.ff_block.w1.weight: TRAINABLE, shape=(512, 128)\n",
      "  net.transformer.layers.4.ff_block.w3.weight: TRAINABLE, shape=(128, 512)\n",
      "  net.transformer.layers.4.ff_block.norm.weight: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.4.ff_block.norm.bias: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.5.attn_block.norm.weight: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.5.attn_block.norm.bias: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.5.attn_block.attention.mha.in_proj_weight: TRAINABLE, shape=(384, 128)\n",
      "  net.transformer.layers.5.attn_block.attention.mha.in_proj_bias: TRAINABLE, shape=(384,)\n",
      "  net.transformer.layers.5.attn_block.attention.mha.out_proj.weight: TRAINABLE, shape=(128, 128)\n",
      "  net.transformer.layers.5.attn_block.attention.mha.out_proj.bias: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.5.ff_block.w1.weight: TRAINABLE, shape=(512, 128)\n",
      "  net.transformer.layers.5.ff_block.w3.weight: TRAINABLE, shape=(128, 512)\n",
      "  net.transformer.layers.5.ff_block.norm.weight: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.5.ff_block.norm.bias: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.6.attn_block.norm.weight: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.6.attn_block.norm.bias: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.6.attn_block.attention.mha.in_proj_weight: TRAINABLE, shape=(384, 128)\n",
      "  net.transformer.layers.6.attn_block.attention.mha.in_proj_bias: TRAINABLE, shape=(384,)\n",
      "  net.transformer.layers.6.attn_block.attention.mha.out_proj.weight: TRAINABLE, shape=(128, 128)\n",
      "  net.transformer.layers.6.attn_block.attention.mha.out_proj.bias: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.6.ff_block.w1.weight: TRAINABLE, shape=(512, 128)\n",
      "  net.transformer.layers.6.ff_block.w3.weight: TRAINABLE, shape=(128, 512)\n",
      "  net.transformer.layers.6.ff_block.norm.weight: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.6.ff_block.norm.bias: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.7.attn_block.norm.weight: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.7.attn_block.norm.bias: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.7.attn_block.attention.mha.in_proj_weight: TRAINABLE, shape=(384, 128)\n",
      "  net.transformer.layers.7.attn_block.attention.mha.in_proj_bias: TRAINABLE, shape=(384,)\n",
      "  net.transformer.layers.7.attn_block.attention.mha.out_proj.weight: TRAINABLE, shape=(128, 128)\n",
      "  net.transformer.layers.7.attn_block.attention.mha.out_proj.bias: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.7.ff_block.w1.weight: TRAINABLE, shape=(512, 128)\n",
      "  net.transformer.layers.7.ff_block.w3.weight: TRAINABLE, shape=(128, 512)\n",
      "  net.transformer.layers.7.ff_block.norm.weight: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.7.ff_block.norm.bias: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.8.attn_block.norm.weight: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.8.attn_block.norm.bias: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.8.attn_block.attention.mha.in_proj_weight: TRAINABLE, shape=(384, 128)\n",
      "  net.transformer.layers.8.attn_block.attention.mha.in_proj_bias: TRAINABLE, shape=(384,)\n",
      "  net.transformer.layers.8.attn_block.attention.mha.out_proj.weight: TRAINABLE, shape=(128, 128)\n",
      "  net.transformer.layers.8.attn_block.attention.mha.out_proj.bias: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.8.ff_block.w1.weight: TRAINABLE, shape=(512, 128)\n",
      "  net.transformer.layers.8.ff_block.w3.weight: TRAINABLE, shape=(128, 512)\n",
      "  net.transformer.layers.8.ff_block.norm.weight: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.8.ff_block.norm.bias: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.9.attn_block.norm.weight: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.9.attn_block.norm.bias: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.9.attn_block.attention.mha.in_proj_weight: TRAINABLE, shape=(384, 128)\n",
      "  net.transformer.layers.9.attn_block.attention.mha.in_proj_bias: TRAINABLE, shape=(384,)\n",
      "  net.transformer.layers.9.attn_block.attention.mha.out_proj.weight: TRAINABLE, shape=(128, 128)\n",
      "  net.transformer.layers.9.attn_block.attention.mha.out_proj.bias: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.9.ff_block.w1.weight: TRAINABLE, shape=(512, 128)\n",
      "  net.transformer.layers.9.ff_block.w3.weight: TRAINABLE, shape=(128, 512)\n",
      "  net.transformer.layers.9.ff_block.norm.weight: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.9.ff_block.norm.bias: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.10.attn_block.norm.weight: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.10.attn_block.norm.bias: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.10.attn_block.attention.mha.in_proj_weight: TRAINABLE, shape=(384, 128)\n",
      "  net.transformer.layers.10.attn_block.attention.mha.in_proj_bias: TRAINABLE, shape=(384,)\n",
      "  net.transformer.layers.10.attn_block.attention.mha.out_proj.weight: TRAINABLE, shape=(128, 128)\n",
      "  net.transformer.layers.10.attn_block.attention.mha.out_proj.bias: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.10.ff_block.w1.weight: TRAINABLE, shape=(512, 128)\n",
      "  net.transformer.layers.10.ff_block.w3.weight: TRAINABLE, shape=(128, 512)\n",
      "  net.transformer.layers.10.ff_block.norm.weight: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.10.ff_block.norm.bias: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.11.attn_block.norm.weight: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.11.attn_block.norm.bias: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.11.attn_block.attention.mha.in_proj_weight: TRAINABLE, shape=(384, 128)\n",
      "  net.transformer.layers.11.attn_block.attention.mha.in_proj_bias: TRAINABLE, shape=(384,)\n",
      "  net.transformer.layers.11.attn_block.attention.mha.out_proj.weight: TRAINABLE, shape=(128, 128)\n",
      "  net.transformer.layers.11.attn_block.attention.mha.out_proj.bias: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.11.ff_block.w1.weight: TRAINABLE, shape=(512, 128)\n",
      "  net.transformer.layers.11.ff_block.w3.weight: TRAINABLE, shape=(128, 512)\n",
      "  net.transformer.layers.11.ff_block.norm.weight: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.11.ff_block.norm.bias: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.12.attn_block.norm.weight: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.12.attn_block.norm.bias: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.12.attn_block.attention.mha.in_proj_weight: TRAINABLE, shape=(384, 128)\n",
      "  net.transformer.layers.12.attn_block.attention.mha.in_proj_bias: TRAINABLE, shape=(384,)\n",
      "  net.transformer.layers.12.attn_block.attention.mha.out_proj.weight: TRAINABLE, shape=(128, 128)\n",
      "  net.transformer.layers.12.attn_block.attention.mha.out_proj.bias: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.12.ff_block.w1.weight: TRAINABLE, shape=(512, 128)\n",
      "  net.transformer.layers.12.ff_block.w3.weight: TRAINABLE, shape=(128, 512)\n",
      "  net.transformer.layers.12.ff_block.norm.weight: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.12.ff_block.norm.bias: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.13.attn_block.norm.weight: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.13.attn_block.norm.bias: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.13.attn_block.attention.mha.in_proj_weight: TRAINABLE, shape=(384, 128)\n",
      "  net.transformer.layers.13.attn_block.attention.mha.in_proj_bias: TRAINABLE, shape=(384,)\n",
      "  net.transformer.layers.13.attn_block.attention.mha.out_proj.weight: TRAINABLE, shape=(128, 128)\n",
      "  net.transformer.layers.13.attn_block.attention.mha.out_proj.bias: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.13.ff_block.w1.weight: TRAINABLE, shape=(512, 128)\n",
      "  net.transformer.layers.13.ff_block.w3.weight: TRAINABLE, shape=(128, 512)\n",
      "  net.transformer.layers.13.ff_block.norm.weight: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.13.ff_block.norm.bias: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.14.attn_block.norm.weight: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.14.attn_block.norm.bias: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.14.attn_block.attention.mha.in_proj_weight: TRAINABLE, shape=(384, 128)\n",
      "  net.transformer.layers.14.attn_block.attention.mha.in_proj_bias: TRAINABLE, shape=(384,)\n",
      "  net.transformer.layers.14.attn_block.attention.mha.out_proj.weight: TRAINABLE, shape=(128, 128)\n",
      "  net.transformer.layers.14.attn_block.attention.mha.out_proj.bias: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.14.ff_block.w1.weight: TRAINABLE, shape=(512, 128)\n",
      "  net.transformer.layers.14.ff_block.w3.weight: TRAINABLE, shape=(128, 512)\n",
      "  net.transformer.layers.14.ff_block.norm.weight: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.14.ff_block.norm.bias: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.15.attn_block.norm.weight: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.15.attn_block.norm.bias: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.15.attn_block.attention.mha.in_proj_weight: TRAINABLE, shape=(384, 128)\n",
      "  net.transformer.layers.15.attn_block.attention.mha.in_proj_bias: TRAINABLE, shape=(384,)\n",
      "  net.transformer.layers.15.attn_block.attention.mha.out_proj.weight: TRAINABLE, shape=(128, 128)\n",
      "  net.transformer.layers.15.attn_block.attention.mha.out_proj.bias: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.15.ff_block.w1.weight: TRAINABLE, shape=(512, 128)\n",
      "  net.transformer.layers.15.ff_block.w3.weight: TRAINABLE, shape=(128, 512)\n",
      "  net.transformer.layers.15.ff_block.norm.weight: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.15.ff_block.norm.bias: TRAINABLE, shape=(128,)\n",
      "  net.transformer.norm.weight: TRAINABLE, shape=(128,)\n",
      "  net.transformer.norm.bias: TRAINABLE, shape=(128,)\n",
      "  net.out_coord_linear.0.weight: TRAINABLE, shape=(128,)\n",
      "  net.out_coord_linear.0.bias: TRAINABLE, shape=(128,)\n",
      "  net.out_coord_linear.1.weight: TRAINABLE, shape=(3, 128)\n",
      "  net.out_atom_type_linear.0.weight: TRAINABLE, shape=(128, 128)\n",
      "  net.out_atom_type_linear.0.bias: TRAINABLE, shape=(128,)\n",
      "  net.out_atom_type_linear.2.weight: TRAINABLE, shape=(9, 128)\n",
      "  net.out_atom_type_linear.2.bias: TRAINABLE, shape=(9,)\n",
      "  net.coord_cross_attention.self_attn.in_proj_weight: TRAINABLE, shape=(384, 128)\n",
      "  net.coord_cross_attention.self_attn.in_proj_bias: TRAINABLE, shape=(384,)\n",
      "  net.coord_cross_attention.self_attn.out_proj.weight: TRAINABLE, shape=(128, 128)\n",
      "  net.coord_cross_attention.self_attn.out_proj.bias: TRAINABLE, shape=(128,)\n",
      "  net.coord_cross_attention.multihead_attn.in_proj_weight: TRAINABLE, shape=(384, 128)\n",
      "  net.coord_cross_attention.multihead_attn.in_proj_bias: TRAINABLE, shape=(384,)\n",
      "  net.coord_cross_attention.multihead_attn.out_proj.weight: TRAINABLE, shape=(128, 128)\n",
      "  net.coord_cross_attention.multihead_attn.out_proj.bias: TRAINABLE, shape=(128,)\n",
      "  net.coord_cross_attention.linear1.weight: TRAINABLE, shape=(512, 128)\n",
      "  net.coord_cross_attention.linear1.bias: TRAINABLE, shape=(512,)\n",
      "  net.coord_cross_attention.linear2.weight: TRAINABLE, shape=(128, 512)\n",
      "  net.coord_cross_attention.linear2.bias: TRAINABLE, shape=(128,)\n",
      "  net.coord_cross_attention.norm1.weight: TRAINABLE, shape=(128,)\n",
      "  net.coord_cross_attention.norm1.bias: TRAINABLE, shape=(128,)\n",
      "  net.coord_cross_attention.norm2.weight: TRAINABLE, shape=(128,)\n",
      "  net.coord_cross_attention.norm2.bias: TRAINABLE, shape=(128,)\n",
      "  net.coord_cross_attention.norm3.weight: TRAINABLE, shape=(128,)\n",
      "  net.coord_cross_attention.norm3.bias: TRAINABLE, shape=(128,)\n",
      "  net.atom_cross_attention.self_attn.in_proj_weight: TRAINABLE, shape=(384, 128)\n",
      "  net.atom_cross_attention.self_attn.in_proj_bias: TRAINABLE, shape=(384,)\n",
      "  net.atom_cross_attention.self_attn.out_proj.weight: TRAINABLE, shape=(128, 128)\n",
      "  net.atom_cross_attention.self_attn.out_proj.bias: TRAINABLE, shape=(128,)\n",
      "  net.atom_cross_attention.multihead_attn.in_proj_weight: TRAINABLE, shape=(384, 128)\n",
      "  net.atom_cross_attention.multihead_attn.in_proj_bias: TRAINABLE, shape=(384,)\n",
      "  net.atom_cross_attention.multihead_attn.out_proj.weight: TRAINABLE, shape=(128, 128)\n",
      "  net.atom_cross_attention.multihead_attn.out_proj.bias: TRAINABLE, shape=(128,)\n",
      "  net.atom_cross_attention.linear1.weight: TRAINABLE, shape=(512, 128)\n",
      "  net.atom_cross_attention.linear1.bias: TRAINABLE, shape=(512,)\n",
      "  net.atom_cross_attention.linear2.weight: TRAINABLE, shape=(128, 512)\n",
      "  net.atom_cross_attention.linear2.bias: TRAINABLE, shape=(128,)\n",
      "  net.atom_cross_attention.norm1.weight: TRAINABLE, shape=(128,)\n",
      "  net.atom_cross_attention.norm1.bias: TRAINABLE, shape=(128,)\n",
      "  net.atom_cross_attention.norm2.weight: TRAINABLE, shape=(128,)\n",
      "  net.atom_cross_attention.norm2.bias: TRAINABLE, shape=(128,)\n",
      "  net.atom_cross_attention.norm3.weight: TRAINABLE, shape=(128,)\n",
      "  net.atom_cross_attention.norm3.bias: TRAINABLE, shape=(128,)\n",
      "  Trainable: 3,711,369\n",
      "  Frozen: 0\n",
      "\n",
      "--- SUMMARY ---\n",
      "Component       Trainable       Frozen        Total\n",
      "--------------------------------------------------\n",
      "net             3,711,369            0    3,711,369\n",
      "--------------------------------------------------\n",
      "TOTAL           3,711,369            0    3,711,369\n"
     ]
    }
   ],
   "source": [
    "check_params(baseline_model, \"Baseline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== REPA ===\n",
      "\n",
      "--- NET ---\n",
      "  net.linear_embed.weight: TRAINABLE, shape=(128, 3)\n",
      "  net.atom_type_embed.weight: TRAINABLE, shape=(9, 128)\n",
      "  net.transformer.layers.0.attn_block.norm.weight: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.0.attn_block.norm.bias: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.0.attn_block.attention.mha.in_proj_weight: TRAINABLE, shape=(384, 128)\n",
      "  net.transformer.layers.0.attn_block.attention.mha.in_proj_bias: TRAINABLE, shape=(384,)\n",
      "  net.transformer.layers.0.attn_block.attention.mha.out_proj.weight: TRAINABLE, shape=(128, 128)\n",
      "  net.transformer.layers.0.attn_block.attention.mha.out_proj.bias: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.0.ff_block.w1.weight: TRAINABLE, shape=(512, 128)\n",
      "  net.transformer.layers.0.ff_block.w3.weight: TRAINABLE, shape=(128, 512)\n",
      "  net.transformer.layers.0.ff_block.norm.weight: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.0.ff_block.norm.bias: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.1.attn_block.norm.weight: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.1.attn_block.norm.bias: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.1.attn_block.attention.mha.in_proj_weight: TRAINABLE, shape=(384, 128)\n",
      "  net.transformer.layers.1.attn_block.attention.mha.in_proj_bias: TRAINABLE, shape=(384,)\n",
      "  net.transformer.layers.1.attn_block.attention.mha.out_proj.weight: TRAINABLE, shape=(128, 128)\n",
      "  net.transformer.layers.1.attn_block.attention.mha.out_proj.bias: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.1.ff_block.w1.weight: TRAINABLE, shape=(512, 128)\n",
      "  net.transformer.layers.1.ff_block.w3.weight: TRAINABLE, shape=(128, 512)\n",
      "  net.transformer.layers.1.ff_block.norm.weight: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.1.ff_block.norm.bias: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.2.attn_block.norm.weight: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.2.attn_block.norm.bias: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.2.attn_block.attention.mha.in_proj_weight: TRAINABLE, shape=(384, 128)\n",
      "  net.transformer.layers.2.attn_block.attention.mha.in_proj_bias: TRAINABLE, shape=(384,)\n",
      "  net.transformer.layers.2.attn_block.attention.mha.out_proj.weight: TRAINABLE, shape=(128, 128)\n",
      "  net.transformer.layers.2.attn_block.attention.mha.out_proj.bias: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.2.ff_block.w1.weight: TRAINABLE, shape=(512, 128)\n",
      "  net.transformer.layers.2.ff_block.w3.weight: TRAINABLE, shape=(128, 512)\n",
      "  net.transformer.layers.2.ff_block.norm.weight: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.2.ff_block.norm.bias: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.3.attn_block.norm.weight: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.3.attn_block.norm.bias: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.3.attn_block.attention.mha.in_proj_weight: TRAINABLE, shape=(384, 128)\n",
      "  net.transformer.layers.3.attn_block.attention.mha.in_proj_bias: TRAINABLE, shape=(384,)\n",
      "  net.transformer.layers.3.attn_block.attention.mha.out_proj.weight: TRAINABLE, shape=(128, 128)\n",
      "  net.transformer.layers.3.attn_block.attention.mha.out_proj.bias: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.3.ff_block.w1.weight: TRAINABLE, shape=(512, 128)\n",
      "  net.transformer.layers.3.ff_block.w3.weight: TRAINABLE, shape=(128, 512)\n",
      "  net.transformer.layers.3.ff_block.norm.weight: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.3.ff_block.norm.bias: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.4.attn_block.norm.weight: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.4.attn_block.norm.bias: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.4.attn_block.attention.mha.in_proj_weight: TRAINABLE, shape=(384, 128)\n",
      "  net.transformer.layers.4.attn_block.attention.mha.in_proj_bias: TRAINABLE, shape=(384,)\n",
      "  net.transformer.layers.4.attn_block.attention.mha.out_proj.weight: TRAINABLE, shape=(128, 128)\n",
      "  net.transformer.layers.4.attn_block.attention.mha.out_proj.bias: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.4.ff_block.w1.weight: TRAINABLE, shape=(512, 128)\n",
      "  net.transformer.layers.4.ff_block.w3.weight: TRAINABLE, shape=(128, 512)\n",
      "  net.transformer.layers.4.ff_block.norm.weight: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.4.ff_block.norm.bias: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.5.attn_block.norm.weight: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.5.attn_block.norm.bias: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.5.attn_block.attention.mha.in_proj_weight: TRAINABLE, shape=(384, 128)\n",
      "  net.transformer.layers.5.attn_block.attention.mha.in_proj_bias: TRAINABLE, shape=(384,)\n",
      "  net.transformer.layers.5.attn_block.attention.mha.out_proj.weight: TRAINABLE, shape=(128, 128)\n",
      "  net.transformer.layers.5.attn_block.attention.mha.out_proj.bias: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.5.ff_block.w1.weight: TRAINABLE, shape=(512, 128)\n",
      "  net.transformer.layers.5.ff_block.w3.weight: TRAINABLE, shape=(128, 512)\n",
      "  net.transformer.layers.5.ff_block.norm.weight: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.5.ff_block.norm.bias: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.6.attn_block.norm.weight: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.6.attn_block.norm.bias: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.6.attn_block.attention.mha.in_proj_weight: TRAINABLE, shape=(384, 128)\n",
      "  net.transformer.layers.6.attn_block.attention.mha.in_proj_bias: TRAINABLE, shape=(384,)\n",
      "  net.transformer.layers.6.attn_block.attention.mha.out_proj.weight: TRAINABLE, shape=(128, 128)\n",
      "  net.transformer.layers.6.attn_block.attention.mha.out_proj.bias: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.6.ff_block.w1.weight: TRAINABLE, shape=(512, 128)\n",
      "  net.transformer.layers.6.ff_block.w3.weight: TRAINABLE, shape=(128, 512)\n",
      "  net.transformer.layers.6.ff_block.norm.weight: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.6.ff_block.norm.bias: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.7.attn_block.norm.weight: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.7.attn_block.norm.bias: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.7.attn_block.attention.mha.in_proj_weight: TRAINABLE, shape=(384, 128)\n",
      "  net.transformer.layers.7.attn_block.attention.mha.in_proj_bias: TRAINABLE, shape=(384,)\n",
      "  net.transformer.layers.7.attn_block.attention.mha.out_proj.weight: TRAINABLE, shape=(128, 128)\n",
      "  net.transformer.layers.7.attn_block.attention.mha.out_proj.bias: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.7.ff_block.w1.weight: TRAINABLE, shape=(512, 128)\n",
      "  net.transformer.layers.7.ff_block.w3.weight: TRAINABLE, shape=(128, 512)\n",
      "  net.transformer.layers.7.ff_block.norm.weight: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.7.ff_block.norm.bias: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.8.attn_block.norm.weight: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.8.attn_block.norm.bias: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.8.attn_block.attention.mha.in_proj_weight: TRAINABLE, shape=(384, 128)\n",
      "  net.transformer.layers.8.attn_block.attention.mha.in_proj_bias: TRAINABLE, shape=(384,)\n",
      "  net.transformer.layers.8.attn_block.attention.mha.out_proj.weight: TRAINABLE, shape=(128, 128)\n",
      "  net.transformer.layers.8.attn_block.attention.mha.out_proj.bias: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.8.ff_block.w1.weight: TRAINABLE, shape=(512, 128)\n",
      "  net.transformer.layers.8.ff_block.w3.weight: TRAINABLE, shape=(128, 512)\n",
      "  net.transformer.layers.8.ff_block.norm.weight: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.8.ff_block.norm.bias: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.9.attn_block.norm.weight: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.9.attn_block.norm.bias: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.9.attn_block.attention.mha.in_proj_weight: TRAINABLE, shape=(384, 128)\n",
      "  net.transformer.layers.9.attn_block.attention.mha.in_proj_bias: TRAINABLE, shape=(384,)\n",
      "  net.transformer.layers.9.attn_block.attention.mha.out_proj.weight: TRAINABLE, shape=(128, 128)\n",
      "  net.transformer.layers.9.attn_block.attention.mha.out_proj.bias: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.9.ff_block.w1.weight: TRAINABLE, shape=(512, 128)\n",
      "  net.transformer.layers.9.ff_block.w3.weight: TRAINABLE, shape=(128, 512)\n",
      "  net.transformer.layers.9.ff_block.norm.weight: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.9.ff_block.norm.bias: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.10.attn_block.norm.weight: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.10.attn_block.norm.bias: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.10.attn_block.attention.mha.in_proj_weight: TRAINABLE, shape=(384, 128)\n",
      "  net.transformer.layers.10.attn_block.attention.mha.in_proj_bias: TRAINABLE, shape=(384,)\n",
      "  net.transformer.layers.10.attn_block.attention.mha.out_proj.weight: TRAINABLE, shape=(128, 128)\n",
      "  net.transformer.layers.10.attn_block.attention.mha.out_proj.bias: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.10.ff_block.w1.weight: TRAINABLE, shape=(512, 128)\n",
      "  net.transformer.layers.10.ff_block.w3.weight: TRAINABLE, shape=(128, 512)\n",
      "  net.transformer.layers.10.ff_block.norm.weight: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.10.ff_block.norm.bias: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.11.attn_block.norm.weight: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.11.attn_block.norm.bias: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.11.attn_block.attention.mha.in_proj_weight: TRAINABLE, shape=(384, 128)\n",
      "  net.transformer.layers.11.attn_block.attention.mha.in_proj_bias: TRAINABLE, shape=(384,)\n",
      "  net.transformer.layers.11.attn_block.attention.mha.out_proj.weight: TRAINABLE, shape=(128, 128)\n",
      "  net.transformer.layers.11.attn_block.attention.mha.out_proj.bias: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.11.ff_block.w1.weight: TRAINABLE, shape=(512, 128)\n",
      "  net.transformer.layers.11.ff_block.w3.weight: TRAINABLE, shape=(128, 512)\n",
      "  net.transformer.layers.11.ff_block.norm.weight: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.11.ff_block.norm.bias: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.12.attn_block.norm.weight: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.12.attn_block.norm.bias: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.12.attn_block.attention.mha.in_proj_weight: TRAINABLE, shape=(384, 128)\n",
      "  net.transformer.layers.12.attn_block.attention.mha.in_proj_bias: TRAINABLE, shape=(384,)\n",
      "  net.transformer.layers.12.attn_block.attention.mha.out_proj.weight: TRAINABLE, shape=(128, 128)\n",
      "  net.transformer.layers.12.attn_block.attention.mha.out_proj.bias: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.12.ff_block.w1.weight: TRAINABLE, shape=(512, 128)\n",
      "  net.transformer.layers.12.ff_block.w3.weight: TRAINABLE, shape=(128, 512)\n",
      "  net.transformer.layers.12.ff_block.norm.weight: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.12.ff_block.norm.bias: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.13.attn_block.norm.weight: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.13.attn_block.norm.bias: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.13.attn_block.attention.mha.in_proj_weight: TRAINABLE, shape=(384, 128)\n",
      "  net.transformer.layers.13.attn_block.attention.mha.in_proj_bias: TRAINABLE, shape=(384,)\n",
      "  net.transformer.layers.13.attn_block.attention.mha.out_proj.weight: TRAINABLE, shape=(128, 128)\n",
      "  net.transformer.layers.13.attn_block.attention.mha.out_proj.bias: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.13.ff_block.w1.weight: TRAINABLE, shape=(512, 128)\n",
      "  net.transformer.layers.13.ff_block.w3.weight: TRAINABLE, shape=(128, 512)\n",
      "  net.transformer.layers.13.ff_block.norm.weight: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.13.ff_block.norm.bias: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.14.attn_block.norm.weight: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.14.attn_block.norm.bias: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.14.attn_block.attention.mha.in_proj_weight: TRAINABLE, shape=(384, 128)\n",
      "  net.transformer.layers.14.attn_block.attention.mha.in_proj_bias: TRAINABLE, shape=(384,)\n",
      "  net.transformer.layers.14.attn_block.attention.mha.out_proj.weight: TRAINABLE, shape=(128, 128)\n",
      "  net.transformer.layers.14.attn_block.attention.mha.out_proj.bias: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.14.ff_block.w1.weight: TRAINABLE, shape=(512, 128)\n",
      "  net.transformer.layers.14.ff_block.w3.weight: TRAINABLE, shape=(128, 512)\n",
      "  net.transformer.layers.14.ff_block.norm.weight: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.14.ff_block.norm.bias: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.15.attn_block.norm.weight: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.15.attn_block.norm.bias: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.15.attn_block.attention.mha.in_proj_weight: TRAINABLE, shape=(384, 128)\n",
      "  net.transformer.layers.15.attn_block.attention.mha.in_proj_bias: TRAINABLE, shape=(384,)\n",
      "  net.transformer.layers.15.attn_block.attention.mha.out_proj.weight: TRAINABLE, shape=(128, 128)\n",
      "  net.transformer.layers.15.attn_block.attention.mha.out_proj.bias: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.15.ff_block.w1.weight: TRAINABLE, shape=(512, 128)\n",
      "  net.transformer.layers.15.ff_block.w3.weight: TRAINABLE, shape=(128, 512)\n",
      "  net.transformer.layers.15.ff_block.norm.weight: TRAINABLE, shape=(128,)\n",
      "  net.transformer.layers.15.ff_block.norm.bias: TRAINABLE, shape=(128,)\n",
      "  net.transformer.norm.weight: TRAINABLE, shape=(128,)\n",
      "  net.transformer.norm.bias: TRAINABLE, shape=(128,)\n",
      "  net.out_coord_linear.0.weight: TRAINABLE, shape=(128,)\n",
      "  net.out_coord_linear.0.bias: TRAINABLE, shape=(128,)\n",
      "  net.out_coord_linear.1.weight: TRAINABLE, shape=(3, 128)\n",
      "  net.out_atom_type_linear.0.weight: TRAINABLE, shape=(128, 128)\n",
      "  net.out_atom_type_linear.0.bias: TRAINABLE, shape=(128,)\n",
      "  net.out_atom_type_linear.2.weight: TRAINABLE, shape=(9, 128)\n",
      "  net.out_atom_type_linear.2.bias: TRAINABLE, shape=(9,)\n",
      "  net.coord_cross_attention.self_attn.in_proj_weight: TRAINABLE, shape=(384, 128)\n",
      "  net.coord_cross_attention.self_attn.in_proj_bias: TRAINABLE, shape=(384,)\n",
      "  net.coord_cross_attention.self_attn.out_proj.weight: TRAINABLE, shape=(128, 128)\n",
      "  net.coord_cross_attention.self_attn.out_proj.bias: TRAINABLE, shape=(128,)\n",
      "  net.coord_cross_attention.multihead_attn.in_proj_weight: TRAINABLE, shape=(384, 128)\n",
      "  net.coord_cross_attention.multihead_attn.in_proj_bias: TRAINABLE, shape=(384,)\n",
      "  net.coord_cross_attention.multihead_attn.out_proj.weight: TRAINABLE, shape=(128, 128)\n",
      "  net.coord_cross_attention.multihead_attn.out_proj.bias: TRAINABLE, shape=(128,)\n",
      "  net.coord_cross_attention.linear1.weight: TRAINABLE, shape=(512, 128)\n",
      "  net.coord_cross_attention.linear1.bias: TRAINABLE, shape=(512,)\n",
      "  net.coord_cross_attention.linear2.weight: TRAINABLE, shape=(128, 512)\n",
      "  net.coord_cross_attention.linear2.bias: TRAINABLE, shape=(128,)\n",
      "  net.coord_cross_attention.norm1.weight: TRAINABLE, shape=(128,)\n",
      "  net.coord_cross_attention.norm1.bias: TRAINABLE, shape=(128,)\n",
      "  net.coord_cross_attention.norm2.weight: TRAINABLE, shape=(128,)\n",
      "  net.coord_cross_attention.norm2.bias: TRAINABLE, shape=(128,)\n",
      "  net.coord_cross_attention.norm3.weight: TRAINABLE, shape=(128,)\n",
      "  net.coord_cross_attention.norm3.bias: TRAINABLE, shape=(128,)\n",
      "  net.atom_cross_attention.self_attn.in_proj_weight: TRAINABLE, shape=(384, 128)\n",
      "  net.atom_cross_attention.self_attn.in_proj_bias: TRAINABLE, shape=(384,)\n",
      "  net.atom_cross_attention.self_attn.out_proj.weight: TRAINABLE, shape=(128, 128)\n",
      "  net.atom_cross_attention.self_attn.out_proj.bias: TRAINABLE, shape=(128,)\n",
      "  net.atom_cross_attention.multihead_attn.in_proj_weight: TRAINABLE, shape=(384, 128)\n",
      "  net.atom_cross_attention.multihead_attn.in_proj_bias: TRAINABLE, shape=(384,)\n",
      "  net.atom_cross_attention.multihead_attn.out_proj.weight: TRAINABLE, shape=(128, 128)\n",
      "  net.atom_cross_attention.multihead_attn.out_proj.bias: TRAINABLE, shape=(128,)\n",
      "  net.atom_cross_attention.linear1.weight: TRAINABLE, shape=(512, 128)\n",
      "  net.atom_cross_attention.linear1.bias: TRAINABLE, shape=(512,)\n",
      "  net.atom_cross_attention.linear2.weight: TRAINABLE, shape=(128, 512)\n",
      "  net.atom_cross_attention.linear2.bias: TRAINABLE, shape=(128,)\n",
      "  net.atom_cross_attention.norm1.weight: TRAINABLE, shape=(128,)\n",
      "  net.atom_cross_attention.norm1.bias: TRAINABLE, shape=(128,)\n",
      "  net.atom_cross_attention.norm2.weight: TRAINABLE, shape=(128,)\n",
      "  net.atom_cross_attention.norm2.bias: TRAINABLE, shape=(128,)\n",
      "  net.atom_cross_attention.norm3.weight: TRAINABLE, shape=(128,)\n",
      "  net.atom_cross_attention.norm3.bias: TRAINABLE, shape=(128,)\n",
      "  Trainable: 3,711,369\n",
      "  Frozen: 0\n",
      "\n",
      "--- PROJECTOR ---\n",
      "  repa_loss.projector.mlp.0.weight: TRAINABLE, shape=(128, 128)\n",
      "  repa_loss.projector.mlp.0.bias: TRAINABLE, shape=(128,)\n",
      "  repa_loss.projector.mlp.2.weight: TRAINABLE, shape=(2048, 128)\n",
      "  repa_loss.projector.mlp.2.bias: TRAINABLE, shape=(2048,)\n",
      "  Trainable: 280,704\n",
      "  Frozen: 0\n",
      "\n",
      "--- ENCODER ---\n",
      "  repa_loss.encoder.message_passing.W_i.weight: FROZEN, shape=(2048, 86)\n",
      "  repa_loss.encoder.message_passing.W_h.weight: FROZEN, shape=(2048, 2048)\n",
      "  repa_loss.encoder.message_passing.W_o.weight: FROZEN, shape=(2048, 2120)\n",
      "  repa_loss.encoder.message_passing.W_o.bias: FROZEN, shape=(2048,)\n",
      "  Trainable: 0\n",
      "  Frozen: 8,714,240\n",
      "\n",
      "--- SUMMARY ---\n",
      "Component       Trainable       Frozen        Total\n",
      "--------------------------------------------------\n",
      "net             3,711,369            0    3,711,369\n",
      "projector         280,704            0      280,704\n",
      "encoder                 0    8,714,240    8,714,240\n",
      "--------------------------------------------------\n",
      "TOTAL           3,992,073    8,714,240   12,706,313\n"
     ]
    }
   ],
   "source": [
    "check_params(repa_model, \"REPA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Forward Pass Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDict(\n",
       "    fields={\n",
       "        atomics: Tensor(shape=torch.Size([4, 9, 9]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        coords: Tensor(shape=torch.Size([4, 9, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        index: Tensor(shape=torch.Size([4]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "        padding_mask: Tensor(shape=torch.Size([4, 9]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
       "    batch_size=torch.Size([4]),\n",
       "    device=None,\n",
       "    is_shared=False)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline loss: 2.8740\n",
      "Baseline stats: {'atomics_loss': tensor(2.2343), 'coords_loss': tensor(0.6396), 'coords_loss_bin_0': nan, 'coords_loss_bin_1': 0.6397254467010498, 'coords_loss_bin_2': nan, 'coords_loss_bin_3': nan, 'coords_loss_bin_4': 0.6396215558052063, 'atomics_logit_norm': 0.7847563028335571, 'atomics_logit_max': 0.4984219968318939, 'atomics_logit_min': -0.2885519564151764, 'coords_logit_norm': 0.9214800596237183}\n"
     ]
    }
   ],
   "source": [
    "# Baseline forward\n",
    "with torch.no_grad():\n",
    "    torch.manual_seed(42)\n",
    "    loss_baseline, stats_baseline = baseline_model(batch, compute_stats=True)\n",
    "\n",
    "print(f\"Baseline loss: {loss_baseline.item():.4f}\")\n",
    "print(f\"Baseline stats: {stats_baseline}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPA loss: 2.8758\n",
      "REPA stats: {'atomics_loss': tensor(2.2343), 'coords_loss': tensor(0.6396), 'coords_loss_bin_0': nan, 'coords_loss_bin_1': 0.6397254467010498, 'coords_loss_bin_2': nan, 'coords_loss_bin_3': nan, 'coords_loss_bin_4': 0.6396215558052063, 'repa_loss': 0.00180918222758919, 'repa_alignment': -0.010839849710464478, 'atomics_logit_norm': 0.7847563028335571, 'atomics_logit_max': 0.4984219968318939, 'atomics_logit_min': -0.2885519564151764, 'coords_logit_norm': 0.9214800596237183}\n",
      "\n",
      "REPA loss component: 0.0018\n",
      "REPA alignment: -0.0108\n"
     ]
    }
   ],
   "source": [
    "# REPA forward\n",
    "with torch.no_grad():\n",
    "    torch.manual_seed(42)\n",
    "    loss_repa, stats_repa = repa_model(batch, compute_stats=True)\n",
    "\n",
    "print(f\"REPA loss: {loss_repa.item():.4f}\")\n",
    "print(f\"REPA stats: {stats_repa}\")\n",
    "\n",
    "# Check REPA-specific stats\n",
    "if \"repa_loss\" in stats_repa:\n",
    "    print(f\"\\nREPA loss component: {stats_repa['repa_loss']:.4f}\")\n",
    "if \"repa_alignment\" in stats_repa:\n",
    "    print(f\"REPA alignment: {stats_repa['repa_alignment']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Backward Pass - Check Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_gradients(model, name):\n",
    "    print(f\"\\n=== {name} Gradients ===\")\n",
    "    has_grad = []\n",
    "    no_grad = []\n",
    "    for n, p in model.named_parameters():\n",
    "        if p.grad is not None:\n",
    "            grad_norm = p.grad.abs().sum().item()\n",
    "            has_grad.append((n, grad_norm))\n",
    "        else:\n",
    "            no_grad.append(n)\n",
    "\n",
    "    print(\"\\nParameters WITH gradients:\")\n",
    "    for n, g in has_grad:\n",
    "        print(f\"  {n}: grad_norm={g:.6f}\")\n",
    "\n",
    "    print(f\"\\nParameters WITHOUT gradients ({len(no_grad)}):\")\n",
    "    for n in no_grad:\n",
    "        print(f\"  {n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Baseline Gradients ===\n",
      "\n",
      "Parameters WITH gradients:\n",
      "  net.linear_embed.weight: grad_norm=1.139927\n",
      "  net.atom_type_embed.weight: grad_norm=2.489598\n",
      "  net.transformer.layers.0.attn_block.norm.weight: grad_norm=0.514989\n",
      "  net.transformer.layers.0.attn_block.norm.bias: grad_norm=0.854234\n",
      "  net.transformer.layers.0.attn_block.attention.mha.in_proj_weight: grad_norm=96.518967\n",
      "  net.transformer.layers.0.attn_block.attention.mha.in_proj_bias: grad_norm=1.203778\n",
      "  net.transformer.layers.0.attn_block.attention.mha.out_proj.weight: grad_norm=100.273125\n",
      "  net.transformer.layers.0.attn_block.attention.mha.out_proj.bias: grad_norm=1.869639\n",
      "  net.transformer.layers.0.ff_block.w1.weight: grad_norm=88.517998\n",
      "  net.transformer.layers.0.ff_block.w3.weight: grad_norm=172.327271\n",
      "  net.transformer.layers.0.ff_block.norm.weight: grad_norm=0.222841\n",
      "  net.transformer.layers.0.ff_block.norm.bias: grad_norm=0.313841\n",
      "  net.transformer.layers.1.attn_block.norm.weight: grad_norm=0.467016\n",
      "  net.transformer.layers.1.attn_block.norm.bias: grad_norm=0.734429\n",
      "  net.transformer.layers.1.attn_block.attention.mha.in_proj_weight: grad_norm=94.187981\n",
      "  net.transformer.layers.1.attn_block.attention.mha.in_proj_bias: grad_norm=1.117166\n",
      "  net.transformer.layers.1.attn_block.attention.mha.out_proj.weight: grad_norm=99.701057\n",
      "  net.transformer.layers.1.attn_block.attention.mha.out_proj.bias: grad_norm=1.807131\n",
      "  net.transformer.layers.1.ff_block.w1.weight: grad_norm=89.835632\n",
      "  net.transformer.layers.1.ff_block.w3.weight: grad_norm=174.265884\n",
      "  net.transformer.layers.1.ff_block.norm.weight: grad_norm=0.242779\n",
      "  net.transformer.layers.1.ff_block.norm.bias: grad_norm=0.342385\n",
      "  net.transformer.layers.2.attn_block.norm.weight: grad_norm=0.414862\n",
      "  net.transformer.layers.2.attn_block.norm.bias: grad_norm=0.699428\n",
      "  net.transformer.layers.2.attn_block.attention.mha.in_proj_weight: grad_norm=78.867165\n",
      "  net.transformer.layers.2.attn_block.attention.mha.in_proj_bias: grad_norm=0.953295\n",
      "  net.transformer.layers.2.attn_block.attention.mha.out_proj.weight: grad_norm=101.544098\n",
      "  net.transformer.layers.2.attn_block.attention.mha.out_proj.bias: grad_norm=1.729161\n",
      "  net.transformer.layers.2.ff_block.w1.weight: grad_norm=89.087128\n",
      "  net.transformer.layers.2.ff_block.w3.weight: grad_norm=169.445969\n",
      "  net.transformer.layers.2.ff_block.norm.weight: grad_norm=0.211890\n",
      "  net.transformer.layers.2.ff_block.norm.bias: grad_norm=0.323403\n",
      "  net.transformer.layers.3.attn_block.norm.weight: grad_norm=0.373410\n",
      "  net.transformer.layers.3.attn_block.norm.bias: grad_norm=0.601339\n",
      "  net.transformer.layers.3.attn_block.attention.mha.in_proj_weight: grad_norm=75.426559\n",
      "  net.transformer.layers.3.attn_block.attention.mha.in_proj_bias: grad_norm=0.922799\n",
      "  net.transformer.layers.3.attn_block.attention.mha.out_proj.weight: grad_norm=81.412987\n",
      "  net.transformer.layers.3.attn_block.attention.mha.out_proj.bias: grad_norm=1.611812\n",
      "  net.transformer.layers.3.ff_block.w1.weight: grad_norm=87.715736\n",
      "  net.transformer.layers.3.ff_block.w3.weight: grad_norm=157.758499\n",
      "  net.transformer.layers.3.ff_block.norm.weight: grad_norm=0.234579\n",
      "  net.transformer.layers.3.ff_block.norm.bias: grad_norm=0.338465\n",
      "  net.transformer.layers.4.attn_block.norm.weight: grad_norm=0.330703\n",
      "  net.transformer.layers.4.attn_block.norm.bias: grad_norm=0.629440\n",
      "  net.transformer.layers.4.attn_block.attention.mha.in_proj_weight: grad_norm=77.665169\n",
      "  net.transformer.layers.4.attn_block.attention.mha.in_proj_bias: grad_norm=0.937650\n",
      "  net.transformer.layers.4.attn_block.attention.mha.out_proj.weight: grad_norm=88.785690\n",
      "  net.transformer.layers.4.attn_block.attention.mha.out_proj.bias: grad_norm=1.542780\n",
      "  net.transformer.layers.4.ff_block.w1.weight: grad_norm=77.618263\n",
      "  net.transformer.layers.4.ff_block.w3.weight: grad_norm=164.640762\n",
      "  net.transformer.layers.4.ff_block.norm.weight: grad_norm=0.217223\n",
      "  net.transformer.layers.4.ff_block.norm.bias: grad_norm=0.299383\n",
      "  net.transformer.layers.5.attn_block.norm.weight: grad_norm=0.405268\n",
      "  net.transformer.layers.5.attn_block.norm.bias: grad_norm=0.670350\n",
      "  net.transformer.layers.5.attn_block.attention.mha.in_proj_weight: grad_norm=75.660210\n",
      "  net.transformer.layers.5.attn_block.attention.mha.in_proj_bias: grad_norm=0.893890\n",
      "  net.transformer.layers.5.attn_block.attention.mha.out_proj.weight: grad_norm=80.244980\n",
      "  net.transformer.layers.5.attn_block.attention.mha.out_proj.bias: grad_norm=1.509466\n",
      "  net.transformer.layers.5.ff_block.w1.weight: grad_norm=76.072098\n",
      "  net.transformer.layers.5.ff_block.w3.weight: grad_norm=143.943085\n",
      "  net.transformer.layers.5.ff_block.norm.weight: grad_norm=0.193909\n",
      "  net.transformer.layers.5.ff_block.norm.bias: grad_norm=0.288118\n",
      "  net.transformer.layers.6.attn_block.norm.weight: grad_norm=0.323366\n",
      "  net.transformer.layers.6.attn_block.norm.bias: grad_norm=0.547043\n",
      "  net.transformer.layers.6.attn_block.attention.mha.in_proj_weight: grad_norm=71.225136\n",
      "  net.transformer.layers.6.attn_block.attention.mha.in_proj_bias: grad_norm=0.856721\n",
      "  net.transformer.layers.6.attn_block.attention.mha.out_proj.weight: grad_norm=83.595184\n",
      "  net.transformer.layers.6.attn_block.attention.mha.out_proj.bias: grad_norm=1.486794\n",
      "  net.transformer.layers.6.ff_block.w1.weight: grad_norm=76.017960\n",
      "  net.transformer.layers.6.ff_block.w3.weight: grad_norm=151.671387\n",
      "  net.transformer.layers.6.ff_block.norm.weight: grad_norm=0.192436\n",
      "  net.transformer.layers.6.ff_block.norm.bias: grad_norm=0.308396\n",
      "  net.transformer.layers.7.attn_block.norm.weight: grad_norm=0.426816\n",
      "  net.transformer.layers.7.attn_block.norm.bias: grad_norm=0.710087\n",
      "  net.transformer.layers.7.attn_block.attention.mha.in_proj_weight: grad_norm=78.523346\n",
      "  net.transformer.layers.7.attn_block.attention.mha.in_proj_bias: grad_norm=0.930511\n",
      "  net.transformer.layers.7.attn_block.attention.mha.out_proj.weight: grad_norm=81.262474\n",
      "  net.transformer.layers.7.attn_block.attention.mha.out_proj.bias: grad_norm=1.494244\n",
      "  net.transformer.layers.7.ff_block.w1.weight: grad_norm=73.287880\n",
      "  net.transformer.layers.7.ff_block.w3.weight: grad_norm=146.965271\n",
      "  net.transformer.layers.7.ff_block.norm.weight: grad_norm=0.173873\n",
      "  net.transformer.layers.7.ff_block.norm.bias: grad_norm=0.273545\n",
      "  net.transformer.layers.8.attn_block.norm.weight: grad_norm=0.392679\n",
      "  net.transformer.layers.8.attn_block.norm.bias: grad_norm=0.572483\n",
      "  net.transformer.layers.8.attn_block.attention.mha.in_proj_weight: grad_norm=72.420738\n",
      "  net.transformer.layers.8.attn_block.attention.mha.in_proj_bias: grad_norm=0.846676\n",
      "  net.transformer.layers.8.attn_block.attention.mha.out_proj.weight: grad_norm=70.566132\n",
      "  net.transformer.layers.8.attn_block.attention.mha.out_proj.bias: grad_norm=1.398565\n",
      "  net.transformer.layers.8.ff_block.w1.weight: grad_norm=74.698555\n",
      "  net.transformer.layers.8.ff_block.w3.weight: grad_norm=141.364227\n",
      "  net.transformer.layers.8.ff_block.norm.weight: grad_norm=0.198740\n",
      "  net.transformer.layers.8.ff_block.norm.bias: grad_norm=0.300553\n",
      "  net.transformer.layers.9.attn_block.norm.weight: grad_norm=0.400198\n",
      "  net.transformer.layers.9.attn_block.norm.bias: grad_norm=0.589088\n",
      "  net.transformer.layers.9.attn_block.attention.mha.in_proj_weight: grad_norm=71.416740\n",
      "  net.transformer.layers.9.attn_block.attention.mha.in_proj_bias: grad_norm=0.838416\n",
      "  net.transformer.layers.9.attn_block.attention.mha.out_proj.weight: grad_norm=67.363686\n",
      "  net.transformer.layers.9.attn_block.attention.mha.out_proj.bias: grad_norm=1.334783\n",
      "  net.transformer.layers.9.ff_block.w1.weight: grad_norm=72.206085\n",
      "  net.transformer.layers.9.ff_block.w3.weight: grad_norm=135.407837\n",
      "  net.transformer.layers.9.ff_block.norm.weight: grad_norm=0.184296\n",
      "  net.transformer.layers.9.ff_block.norm.bias: grad_norm=0.259084\n",
      "  net.transformer.layers.10.attn_block.norm.weight: grad_norm=0.329819\n",
      "  net.transformer.layers.10.attn_block.norm.bias: grad_norm=0.511761\n",
      "  net.transformer.layers.10.attn_block.attention.mha.in_proj_weight: grad_norm=65.972816\n",
      "  net.transformer.layers.10.attn_block.attention.mha.in_proj_bias: grad_norm=0.763487\n",
      "  net.transformer.layers.10.attn_block.attention.mha.out_proj.weight: grad_norm=76.164192\n",
      "  net.transformer.layers.10.attn_block.attention.mha.out_proj.bias: grad_norm=1.301207\n",
      "  net.transformer.layers.10.ff_block.w1.weight: grad_norm=71.608810\n",
      "  net.transformer.layers.10.ff_block.w3.weight: grad_norm=136.608902\n",
      "  net.transformer.layers.10.ff_block.norm.weight: grad_norm=0.194551\n",
      "  net.transformer.layers.10.ff_block.norm.bias: grad_norm=0.269942\n",
      "  net.transformer.layers.11.attn_block.norm.weight: grad_norm=0.314915\n",
      "  net.transformer.layers.11.attn_block.norm.bias: grad_norm=0.514363\n",
      "  net.transformer.layers.11.attn_block.attention.mha.in_proj_weight: grad_norm=66.321884\n",
      "  net.transformer.layers.11.attn_block.attention.mha.in_proj_bias: grad_norm=0.770261\n",
      "  net.transformer.layers.11.attn_block.attention.mha.out_proj.weight: grad_norm=65.003128\n",
      "  net.transformer.layers.11.attn_block.attention.mha.out_proj.bias: grad_norm=1.261962\n",
      "  net.transformer.layers.11.ff_block.w1.weight: grad_norm=68.946152\n",
      "  net.transformer.layers.11.ff_block.w3.weight: grad_norm=133.360901\n",
      "  net.transformer.layers.11.ff_block.norm.weight: grad_norm=0.164208\n",
      "  net.transformer.layers.11.ff_block.norm.bias: grad_norm=0.267378\n",
      "  net.transformer.layers.12.attn_block.norm.weight: grad_norm=0.286816\n",
      "  net.transformer.layers.12.attn_block.norm.bias: grad_norm=0.453697\n",
      "  net.transformer.layers.12.attn_block.attention.mha.in_proj_weight: grad_norm=63.946926\n",
      "  net.transformer.layers.12.attn_block.attention.mha.in_proj_bias: grad_norm=0.750266\n",
      "  net.transformer.layers.12.attn_block.attention.mha.out_proj.weight: grad_norm=67.627113\n",
      "  net.transformer.layers.12.attn_block.attention.mha.out_proj.bias: grad_norm=1.227778\n",
      "  net.transformer.layers.12.ff_block.w1.weight: grad_norm=63.339798\n",
      "  net.transformer.layers.12.ff_block.w3.weight: grad_norm=129.450821\n",
      "  net.transformer.layers.12.ff_block.norm.weight: grad_norm=0.162126\n",
      "  net.transformer.layers.12.ff_block.norm.bias: grad_norm=0.211325\n",
      "  net.transformer.layers.13.attn_block.norm.weight: grad_norm=0.297831\n",
      "  net.transformer.layers.13.attn_block.norm.bias: grad_norm=0.486045\n",
      "  net.transformer.layers.13.attn_block.attention.mha.in_proj_weight: grad_norm=60.937798\n",
      "  net.transformer.layers.13.attn_block.attention.mha.in_proj_bias: grad_norm=0.718504\n",
      "  net.transformer.layers.13.attn_block.attention.mha.out_proj.weight: grad_norm=68.681747\n",
      "  net.transformer.layers.13.attn_block.attention.mha.out_proj.bias: grad_norm=1.169896\n",
      "  net.transformer.layers.13.ff_block.w1.weight: grad_norm=61.808426\n",
      "  net.transformer.layers.13.ff_block.w3.weight: grad_norm=135.028656\n",
      "  net.transformer.layers.13.ff_block.norm.weight: grad_norm=0.164462\n",
      "  net.transformer.layers.13.ff_block.norm.bias: grad_norm=0.218651\n",
      "  net.transformer.layers.14.attn_block.norm.weight: grad_norm=0.302343\n",
      "  net.transformer.layers.14.attn_block.norm.bias: grad_norm=0.448558\n",
      "  net.transformer.layers.14.attn_block.attention.mha.in_proj_weight: grad_norm=60.282204\n",
      "  net.transformer.layers.14.attn_block.attention.mha.in_proj_bias: grad_norm=0.707412\n",
      "  net.transformer.layers.14.attn_block.attention.mha.out_proj.weight: grad_norm=69.441856\n",
      "  net.transformer.layers.14.attn_block.attention.mha.out_proj.bias: grad_norm=1.203046\n",
      "  net.transformer.layers.14.ff_block.w1.weight: grad_norm=60.303082\n",
      "  net.transformer.layers.14.ff_block.w3.weight: grad_norm=128.797913\n",
      "  net.transformer.layers.14.ff_block.norm.weight: grad_norm=0.145711\n",
      "  net.transformer.layers.14.ff_block.norm.bias: grad_norm=0.226838\n",
      "  net.transformer.layers.15.attn_block.norm.weight: grad_norm=0.255523\n",
      "  net.transformer.layers.15.attn_block.norm.bias: grad_norm=0.432435\n",
      "  net.transformer.layers.15.attn_block.attention.mha.in_proj_weight: grad_norm=58.695385\n",
      "  net.transformer.layers.15.attn_block.attention.mha.in_proj_bias: grad_norm=0.677306\n",
      "  net.transformer.layers.15.attn_block.attention.mha.out_proj.weight: grad_norm=65.592674\n",
      "  net.transformer.layers.15.attn_block.attention.mha.out_proj.bias: grad_norm=1.175714\n",
      "  net.transformer.layers.15.ff_block.w1.weight: grad_norm=58.189945\n",
      "  net.transformer.layers.15.ff_block.w3.weight: grad_norm=114.078682\n",
      "  net.transformer.layers.15.ff_block.norm.weight: grad_norm=0.160422\n",
      "  net.transformer.layers.15.ff_block.norm.bias: grad_norm=0.219049\n",
      "  net.transformer.norm.weight: grad_norm=1.651837\n",
      "  net.transformer.norm.bias: grad_norm=2.472892\n",
      "  net.out_coord_linear.0.weight: grad_norm=1.717175\n",
      "  net.out_coord_linear.0.bias: grad_norm=2.393392\n",
      "  net.out_coord_linear.1.weight: grad_norm=56.972942\n",
      "  net.out_atom_type_linear.0.weight: grad_norm=201.148132\n",
      "  net.out_atom_type_linear.0.bias: grad_norm=1.833306\n",
      "  net.out_atom_type_linear.2.weight: grad_norm=49.720188\n",
      "  net.out_atom_type_linear.2.bias: grad_norm=1.378653\n",
      "  net.coord_cross_attention.self_attn.in_proj_weight: grad_norm=87.029861\n",
      "  net.coord_cross_attention.self_attn.in_proj_bias: grad_norm=0.924127\n",
      "  net.coord_cross_attention.self_attn.out_proj.weight: grad_norm=122.304764\n",
      "  net.coord_cross_attention.self_attn.out_proj.bias: grad_norm=1.823472\n",
      "  net.coord_cross_attention.multihead_attn.in_proj_weight: grad_norm=152.265335\n",
      "  net.coord_cross_attention.multihead_attn.in_proj_bias: grad_norm=1.189078\n",
      "  net.coord_cross_attention.multihead_attn.out_proj.weight: grad_norm=149.371780\n",
      "  net.coord_cross_attention.multihead_attn.out_proj.bias: grad_norm=1.817048\n",
      "  net.coord_cross_attention.linear1.weight: grad_norm=102.966881\n",
      "  net.coord_cross_attention.linear1.bias: grad_norm=1.060393\n",
      "  net.coord_cross_attention.linear2.weight: grad_norm=226.003845\n",
      "  net.coord_cross_attention.linear2.bias: grad_norm=1.827503\n",
      "  net.coord_cross_attention.norm1.weight: grad_norm=0.425026\n",
      "  net.coord_cross_attention.norm1.bias: grad_norm=0.619511\n",
      "  net.coord_cross_attention.norm2.weight: grad_norm=0.088926\n",
      "  net.coord_cross_attention.norm2.bias: grad_norm=0.124159\n",
      "  net.coord_cross_attention.norm3.weight: grad_norm=0.265967\n",
      "  net.coord_cross_attention.norm3.bias: grad_norm=0.348719\n",
      "  net.atom_cross_attention.self_attn.in_proj_weight: grad_norm=61.938911\n",
      "  net.atom_cross_attention.self_attn.in_proj_bias: grad_norm=0.723263\n",
      "  net.atom_cross_attention.self_attn.out_proj.weight: grad_norm=64.244446\n",
      "  net.atom_cross_attention.self_attn.out_proj.bias: grad_norm=1.136310\n",
      "  net.atom_cross_attention.multihead_attn.in_proj_weight: grad_norm=99.739777\n",
      "  net.atom_cross_attention.multihead_attn.in_proj_bias: grad_norm=0.824008\n",
      "  net.atom_cross_attention.multihead_attn.out_proj.weight: grad_norm=93.238647\n",
      "  net.atom_cross_attention.multihead_attn.out_proj.bias: grad_norm=1.153430\n",
      "  net.atom_cross_attention.linear1.weight: grad_norm=62.777802\n",
      "  net.atom_cross_attention.linear1.bias: grad_norm=0.664385\n",
      "  net.atom_cross_attention.linear2.weight: grad_norm=147.195328\n",
      "  net.atom_cross_attention.linear2.bias: grad_norm=1.144023\n",
      "  net.atom_cross_attention.norm1.weight: grad_norm=0.308183\n",
      "  net.atom_cross_attention.norm1.bias: grad_norm=0.509508\n",
      "  net.atom_cross_attention.norm2.weight: grad_norm=0.066050\n",
      "  net.atom_cross_attention.norm2.bias: grad_norm=0.092263\n",
      "  net.atom_cross_attention.norm3.weight: grad_norm=0.197476\n",
      "  net.atom_cross_attention.norm3.bias: grad_norm=0.266724\n",
      "\n",
      "Parameters WITHOUT gradients (0):\n"
     ]
    }
   ],
   "source": [
    "# Baseline backward\n",
    "baseline_model.zero_grad()\n",
    "torch.manual_seed(42)\n",
    "loss_baseline, _ = baseline_model(batch)\n",
    "loss_baseline.backward()\n",
    "check_gradients(baseline_model, \"Baseline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== REPA Gradients ===\n",
      "\n",
      "Parameters WITH gradients:\n",
      "  net.linear_embed.weight: grad_norm=1.140009\n",
      "  net.atom_type_embed.weight: grad_norm=2.489639\n",
      "  net.transformer.layers.0.attn_block.norm.weight: grad_norm=0.516037\n",
      "  net.transformer.layers.0.attn_block.norm.bias: grad_norm=0.856814\n",
      "  net.transformer.layers.0.attn_block.attention.mha.in_proj_weight: grad_norm=96.504417\n",
      "  net.transformer.layers.0.attn_block.attention.mha.in_proj_bias: grad_norm=1.203212\n",
      "  net.transformer.layers.0.attn_block.attention.mha.out_proj.weight: grad_norm=100.337112\n",
      "  net.transformer.layers.0.attn_block.attention.mha.out_proj.bias: grad_norm=1.869572\n",
      "  net.transformer.layers.0.ff_block.w1.weight: grad_norm=88.641708\n",
      "  net.transformer.layers.0.ff_block.w3.weight: grad_norm=172.379211\n",
      "  net.transformer.layers.0.ff_block.norm.weight: grad_norm=0.223239\n",
      "  net.transformer.layers.0.ff_block.norm.bias: grad_norm=0.315179\n",
      "  net.transformer.layers.1.attn_block.norm.weight: grad_norm=0.469497\n",
      "  net.transformer.layers.1.attn_block.norm.bias: grad_norm=0.737307\n",
      "  net.transformer.layers.1.attn_block.attention.mha.in_proj_weight: grad_norm=94.031982\n",
      "  net.transformer.layers.1.attn_block.attention.mha.in_proj_bias: grad_norm=1.115564\n",
      "  net.transformer.layers.1.attn_block.attention.mha.out_proj.weight: grad_norm=99.844391\n",
      "  net.transformer.layers.1.attn_block.attention.mha.out_proj.bias: grad_norm=1.809175\n",
      "  net.transformer.layers.1.ff_block.w1.weight: grad_norm=89.759781\n",
      "  net.transformer.layers.1.ff_block.w3.weight: grad_norm=174.418152\n",
      "  net.transformer.layers.1.ff_block.norm.weight: grad_norm=0.243069\n",
      "  net.transformer.layers.1.ff_block.norm.bias: grad_norm=0.342275\n",
      "  net.transformer.layers.2.attn_block.norm.weight: grad_norm=0.415215\n",
      "  net.transformer.layers.2.attn_block.norm.bias: grad_norm=0.699266\n",
      "  net.transformer.layers.2.attn_block.attention.mha.in_proj_weight: grad_norm=78.937370\n",
      "  net.transformer.layers.2.attn_block.attention.mha.in_proj_bias: grad_norm=0.956536\n",
      "  net.transformer.layers.2.attn_block.attention.mha.out_proj.weight: grad_norm=101.666565\n",
      "  net.transformer.layers.2.attn_block.attention.mha.out_proj.bias: grad_norm=1.729708\n",
      "  net.transformer.layers.2.ff_block.w1.weight: grad_norm=89.152756\n",
      "  net.transformer.layers.2.ff_block.w3.weight: grad_norm=169.615280\n",
      "  net.transformer.layers.2.ff_block.norm.weight: grad_norm=0.211879\n",
      "  net.transformer.layers.2.ff_block.norm.bias: grad_norm=0.323857\n",
      "  net.transformer.layers.3.attn_block.norm.weight: grad_norm=0.372282\n",
      "  net.transformer.layers.3.attn_block.norm.bias: grad_norm=0.602964\n",
      "  net.transformer.layers.3.attn_block.attention.mha.in_proj_weight: grad_norm=75.432350\n",
      "  net.transformer.layers.3.attn_block.attention.mha.in_proj_bias: grad_norm=0.921618\n",
      "  net.transformer.layers.3.attn_block.attention.mha.out_proj.weight: grad_norm=81.532677\n",
      "  net.transformer.layers.3.attn_block.attention.mha.out_proj.bias: grad_norm=1.614265\n",
      "  net.transformer.layers.3.ff_block.w1.weight: grad_norm=87.728928\n",
      "  net.transformer.layers.3.ff_block.w3.weight: grad_norm=157.907959\n",
      "  net.transformer.layers.3.ff_block.norm.weight: grad_norm=0.234681\n",
      "  net.transformer.layers.3.ff_block.norm.bias: grad_norm=0.337866\n",
      "  net.transformer.layers.4.attn_block.norm.weight: grad_norm=0.329820\n",
      "  net.transformer.layers.4.attn_block.norm.bias: grad_norm=0.628477\n",
      "  net.transformer.layers.4.attn_block.attention.mha.in_proj_weight: grad_norm=77.526863\n",
      "  net.transformer.layers.4.attn_block.attention.mha.in_proj_bias: grad_norm=0.935447\n",
      "  net.transformer.layers.4.attn_block.attention.mha.out_proj.weight: grad_norm=88.938042\n",
      "  net.transformer.layers.4.attn_block.attention.mha.out_proj.bias: grad_norm=1.545939\n",
      "  net.transformer.layers.4.ff_block.w1.weight: grad_norm=77.776993\n",
      "  net.transformer.layers.4.ff_block.w3.weight: grad_norm=164.970291\n",
      "  net.transformer.layers.4.ff_block.norm.weight: grad_norm=0.216775\n",
      "  net.transformer.layers.4.ff_block.norm.bias: grad_norm=0.300296\n",
      "  net.transformer.layers.5.attn_block.norm.weight: grad_norm=0.405462\n",
      "  net.transformer.layers.5.attn_block.norm.bias: grad_norm=0.669394\n",
      "  net.transformer.layers.5.attn_block.attention.mha.in_proj_weight: grad_norm=75.723061\n",
      "  net.transformer.layers.5.attn_block.attention.mha.in_proj_bias: grad_norm=0.895353\n",
      "  net.transformer.layers.5.attn_block.attention.mha.out_proj.weight: grad_norm=80.364990\n",
      "  net.transformer.layers.5.attn_block.attention.mha.out_proj.bias: grad_norm=1.513152\n",
      "  net.transformer.layers.5.ff_block.w1.weight: grad_norm=75.990257\n",
      "  net.transformer.layers.5.ff_block.w3.weight: grad_norm=144.101624\n",
      "  net.transformer.layers.5.ff_block.norm.weight: grad_norm=0.193887\n",
      "  net.transformer.layers.5.ff_block.norm.bias: grad_norm=0.287676\n",
      "  net.transformer.layers.6.attn_block.norm.weight: grad_norm=0.324699\n",
      "  net.transformer.layers.6.attn_block.norm.bias: grad_norm=0.549625\n",
      "  net.transformer.layers.6.attn_block.attention.mha.in_proj_weight: grad_norm=71.421898\n",
      "  net.transformer.layers.6.attn_block.attention.mha.in_proj_bias: grad_norm=0.859448\n",
      "  net.transformer.layers.6.attn_block.attention.mha.out_proj.weight: grad_norm=83.684853\n",
      "  net.transformer.layers.6.attn_block.attention.mha.out_proj.bias: grad_norm=1.489451\n",
      "  net.transformer.layers.6.ff_block.w1.weight: grad_norm=76.127151\n",
      "  net.transformer.layers.6.ff_block.w3.weight: grad_norm=151.727661\n",
      "  net.transformer.layers.6.ff_block.norm.weight: grad_norm=0.192863\n",
      "  net.transformer.layers.6.ff_block.norm.bias: grad_norm=0.308058\n",
      "  net.transformer.layers.7.attn_block.norm.weight: grad_norm=0.426307\n",
      "  net.transformer.layers.7.attn_block.norm.bias: grad_norm=0.710058\n",
      "  net.transformer.layers.7.attn_block.attention.mha.in_proj_weight: grad_norm=78.511940\n",
      "  net.transformer.layers.7.attn_block.attention.mha.in_proj_bias: grad_norm=0.929896\n",
      "  net.transformer.layers.7.attn_block.attention.mha.out_proj.weight: grad_norm=81.293343\n",
      "  net.transformer.layers.7.attn_block.attention.mha.out_proj.bias: grad_norm=1.494903\n",
      "  net.transformer.layers.7.ff_block.w1.weight: grad_norm=73.325890\n",
      "  net.transformer.layers.7.ff_block.w3.weight: grad_norm=147.000061\n",
      "  net.transformer.layers.7.ff_block.norm.weight: grad_norm=0.174614\n",
      "  net.transformer.layers.7.ff_block.norm.bias: grad_norm=0.274373\n",
      "  net.transformer.layers.8.attn_block.norm.weight: grad_norm=0.393464\n",
      "  net.transformer.layers.8.attn_block.norm.bias: grad_norm=0.573312\n",
      "  net.transformer.layers.8.attn_block.attention.mha.in_proj_weight: grad_norm=72.412697\n",
      "  net.transformer.layers.8.attn_block.attention.mha.in_proj_bias: grad_norm=0.846841\n",
      "  net.transformer.layers.8.attn_block.attention.mha.out_proj.weight: grad_norm=70.635872\n",
      "  net.transformer.layers.8.attn_block.attention.mha.out_proj.bias: grad_norm=1.399627\n",
      "  net.transformer.layers.8.ff_block.w1.weight: grad_norm=74.806808\n",
      "  net.transformer.layers.8.ff_block.w3.weight: grad_norm=141.546783\n",
      "  net.transformer.layers.8.ff_block.norm.weight: grad_norm=0.199422\n",
      "  net.transformer.layers.8.ff_block.norm.bias: grad_norm=0.300812\n",
      "  net.transformer.layers.9.attn_block.norm.weight: grad_norm=0.401084\n",
      "  net.transformer.layers.9.attn_block.norm.bias: grad_norm=0.589911\n",
      "  net.transformer.layers.9.attn_block.attention.mha.in_proj_weight: grad_norm=71.300560\n",
      "  net.transformer.layers.9.attn_block.attention.mha.in_proj_bias: grad_norm=0.836396\n",
      "  net.transformer.layers.9.attn_block.attention.mha.out_proj.weight: grad_norm=67.399055\n",
      "  net.transformer.layers.9.attn_block.attention.mha.out_proj.bias: grad_norm=1.335424\n",
      "  net.transformer.layers.9.ff_block.w1.weight: grad_norm=72.166595\n",
      "  net.transformer.layers.9.ff_block.w3.weight: grad_norm=135.575653\n",
      "  net.transformer.layers.9.ff_block.norm.weight: grad_norm=0.184069\n",
      "  net.transformer.layers.9.ff_block.norm.bias: grad_norm=0.258604\n",
      "  net.transformer.layers.10.attn_block.norm.weight: grad_norm=0.330352\n",
      "  net.transformer.layers.10.attn_block.norm.bias: grad_norm=0.511239\n",
      "  net.transformer.layers.10.attn_block.attention.mha.in_proj_weight: grad_norm=65.941818\n",
      "  net.transformer.layers.10.attn_block.attention.mha.in_proj_bias: grad_norm=0.763121\n",
      "  net.transformer.layers.10.attn_block.attention.mha.out_proj.weight: grad_norm=76.233444\n",
      "  net.transformer.layers.10.attn_block.attention.mha.out_proj.bias: grad_norm=1.301077\n",
      "  net.transformer.layers.10.ff_block.w1.weight: grad_norm=71.627708\n",
      "  net.transformer.layers.10.ff_block.w3.weight: grad_norm=136.811798\n",
      "  net.transformer.layers.10.ff_block.norm.weight: grad_norm=0.194908\n",
      "  net.transformer.layers.10.ff_block.norm.bias: grad_norm=0.269560\n",
      "  net.transformer.layers.11.attn_block.norm.weight: grad_norm=0.316936\n",
      "  net.transformer.layers.11.attn_block.norm.bias: grad_norm=0.515364\n",
      "  net.transformer.layers.11.attn_block.attention.mha.in_proj_weight: grad_norm=66.421730\n",
      "  net.transformer.layers.11.attn_block.attention.mha.in_proj_bias: grad_norm=0.771388\n",
      "  net.transformer.layers.11.attn_block.attention.mha.out_proj.weight: grad_norm=65.160156\n",
      "  net.transformer.layers.11.attn_block.attention.mha.out_proj.bias: grad_norm=1.265626\n",
      "  net.transformer.layers.11.ff_block.w1.weight: grad_norm=69.132751\n",
      "  net.transformer.layers.11.ff_block.w3.weight: grad_norm=133.680969\n",
      "  net.transformer.layers.11.ff_block.norm.weight: grad_norm=0.164708\n",
      "  net.transformer.layers.11.ff_block.norm.bias: grad_norm=0.269579\n",
      "  net.transformer.layers.12.attn_block.norm.weight: grad_norm=0.286493\n",
      "  net.transformer.layers.12.attn_block.norm.bias: grad_norm=0.455204\n",
      "  net.transformer.layers.12.attn_block.attention.mha.in_proj_weight: grad_norm=64.055168\n",
      "  net.transformer.layers.12.attn_block.attention.mha.in_proj_bias: grad_norm=0.751289\n",
      "  net.transformer.layers.12.attn_block.attention.mha.out_proj.weight: grad_norm=67.733582\n",
      "  net.transformer.layers.12.attn_block.attention.mha.out_proj.bias: grad_norm=1.229054\n",
      "  net.transformer.layers.12.ff_block.w1.weight: grad_norm=63.432259\n",
      "  net.transformer.layers.12.ff_block.w3.weight: grad_norm=129.610580\n",
      "  net.transformer.layers.12.ff_block.norm.weight: grad_norm=0.162131\n",
      "  net.transformer.layers.12.ff_block.norm.bias: grad_norm=0.212310\n",
      "  net.transformer.layers.13.attn_block.norm.weight: grad_norm=0.296892\n",
      "  net.transformer.layers.13.attn_block.norm.bias: grad_norm=0.485721\n",
      "  net.transformer.layers.13.attn_block.attention.mha.in_proj_weight: grad_norm=60.881001\n",
      "  net.transformer.layers.13.attn_block.attention.mha.in_proj_bias: grad_norm=0.717778\n",
      "  net.transformer.layers.13.attn_block.attention.mha.out_proj.weight: grad_norm=68.748436\n",
      "  net.transformer.layers.13.attn_block.attention.mha.out_proj.bias: grad_norm=1.169999\n",
      "  net.transformer.layers.13.ff_block.w1.weight: grad_norm=61.865227\n",
      "  net.transformer.layers.13.ff_block.w3.weight: grad_norm=135.123764\n",
      "  net.transformer.layers.13.ff_block.norm.weight: grad_norm=0.164039\n",
      "  net.transformer.layers.13.ff_block.norm.bias: grad_norm=0.218346\n",
      "  net.transformer.layers.14.attn_block.norm.weight: grad_norm=0.302704\n",
      "  net.transformer.layers.14.attn_block.norm.bias: grad_norm=0.448911\n",
      "  net.transformer.layers.14.attn_block.attention.mha.in_proj_weight: grad_norm=60.179169\n",
      "  net.transformer.layers.14.attn_block.attention.mha.in_proj_bias: grad_norm=0.705829\n",
      "  net.transformer.layers.14.attn_block.attention.mha.out_proj.weight: grad_norm=69.492599\n",
      "  net.transformer.layers.14.attn_block.attention.mha.out_proj.bias: grad_norm=1.203499\n",
      "  net.transformer.layers.14.ff_block.w1.weight: grad_norm=60.377502\n",
      "  net.transformer.layers.14.ff_block.w3.weight: grad_norm=128.928772\n",
      "  net.transformer.layers.14.ff_block.norm.weight: grad_norm=0.145987\n",
      "  net.transformer.layers.14.ff_block.norm.bias: grad_norm=0.227608\n",
      "  net.transformer.layers.15.attn_block.norm.weight: grad_norm=0.256411\n",
      "  net.transformer.layers.15.attn_block.norm.bias: grad_norm=0.433677\n",
      "  net.transformer.layers.15.attn_block.attention.mha.in_proj_weight: grad_norm=58.705887\n",
      "  net.transformer.layers.15.attn_block.attention.mha.in_proj_bias: grad_norm=0.676551\n",
      "  net.transformer.layers.15.attn_block.attention.mha.out_proj.weight: grad_norm=65.670914\n",
      "  net.transformer.layers.15.attn_block.attention.mha.out_proj.bias: grad_norm=1.178312\n",
      "  net.transformer.layers.15.ff_block.w1.weight: grad_norm=58.368774\n",
      "  net.transformer.layers.15.ff_block.w3.weight: grad_norm=114.221886\n",
      "  net.transformer.layers.15.ff_block.norm.weight: grad_norm=0.161151\n",
      "  net.transformer.layers.15.ff_block.norm.bias: grad_norm=0.219936\n",
      "  net.transformer.norm.weight: grad_norm=1.659414\n",
      "  net.transformer.norm.bias: grad_norm=2.474844\n",
      "  net.out_coord_linear.0.weight: grad_norm=1.717175\n",
      "  net.out_coord_linear.0.bias: grad_norm=2.393392\n",
      "  net.out_coord_linear.1.weight: grad_norm=56.972942\n",
      "  net.out_atom_type_linear.0.weight: grad_norm=201.148132\n",
      "  net.out_atom_type_linear.0.bias: grad_norm=1.833306\n",
      "  net.out_atom_type_linear.2.weight: grad_norm=49.720188\n",
      "  net.out_atom_type_linear.2.bias: grad_norm=1.378653\n",
      "  net.coord_cross_attention.self_attn.in_proj_weight: grad_norm=87.029861\n",
      "  net.coord_cross_attention.self_attn.in_proj_bias: grad_norm=0.924127\n",
      "  net.coord_cross_attention.self_attn.out_proj.weight: grad_norm=122.304764\n",
      "  net.coord_cross_attention.self_attn.out_proj.bias: grad_norm=1.823472\n",
      "  net.coord_cross_attention.multihead_attn.in_proj_weight: grad_norm=152.265335\n",
      "  net.coord_cross_attention.multihead_attn.in_proj_bias: grad_norm=1.189078\n",
      "  net.coord_cross_attention.multihead_attn.out_proj.weight: grad_norm=149.371780\n",
      "  net.coord_cross_attention.multihead_attn.out_proj.bias: grad_norm=1.817048\n",
      "  net.coord_cross_attention.linear1.weight: grad_norm=102.966881\n",
      "  net.coord_cross_attention.linear1.bias: grad_norm=1.060393\n",
      "  net.coord_cross_attention.linear2.weight: grad_norm=226.003845\n",
      "  net.coord_cross_attention.linear2.bias: grad_norm=1.827503\n",
      "  net.coord_cross_attention.norm1.weight: grad_norm=0.425026\n",
      "  net.coord_cross_attention.norm1.bias: grad_norm=0.619511\n",
      "  net.coord_cross_attention.norm2.weight: grad_norm=0.088926\n",
      "  net.coord_cross_attention.norm2.bias: grad_norm=0.124159\n",
      "  net.coord_cross_attention.norm3.weight: grad_norm=0.265967\n",
      "  net.coord_cross_attention.norm3.bias: grad_norm=0.348719\n",
      "  net.atom_cross_attention.self_attn.in_proj_weight: grad_norm=61.938911\n",
      "  net.atom_cross_attention.self_attn.in_proj_bias: grad_norm=0.723263\n",
      "  net.atom_cross_attention.self_attn.out_proj.weight: grad_norm=64.244446\n",
      "  net.atom_cross_attention.self_attn.out_proj.bias: grad_norm=1.136310\n",
      "  net.atom_cross_attention.multihead_attn.in_proj_weight: grad_norm=99.739777\n",
      "  net.atom_cross_attention.multihead_attn.in_proj_bias: grad_norm=0.824008\n",
      "  net.atom_cross_attention.multihead_attn.out_proj.weight: grad_norm=93.238647\n",
      "  net.atom_cross_attention.multihead_attn.out_proj.bias: grad_norm=1.153430\n",
      "  net.atom_cross_attention.linear1.weight: grad_norm=62.777802\n",
      "  net.atom_cross_attention.linear1.bias: grad_norm=0.664385\n",
      "  net.atom_cross_attention.linear2.weight: grad_norm=147.195328\n",
      "  net.atom_cross_attention.linear2.bias: grad_norm=1.144023\n",
      "  net.atom_cross_attention.norm1.weight: grad_norm=0.308183\n",
      "  net.atom_cross_attention.norm1.bias: grad_norm=0.509508\n",
      "  net.atom_cross_attention.norm2.weight: grad_norm=0.066050\n",
      "  net.atom_cross_attention.norm2.bias: grad_norm=0.092263\n",
      "  net.atom_cross_attention.norm3.weight: grad_norm=0.197476\n",
      "  net.atom_cross_attention.norm3.bias: grad_norm=0.266724\n",
      "  repa_loss.projector.mlp.0.weight: grad_norm=6.846203\n",
      "  repa_loss.projector.mlp.0.bias: grad_norm=0.078631\n",
      "  repa_loss.projector.mlp.2.weight: grad_norm=14.461800\n",
      "  repa_loss.projector.mlp.2.bias: grad_norm=0.555969\n",
      "\n",
      "Parameters WITHOUT gradients (4):\n",
      "  repa_loss.encoder.message_passing.W_i.weight\n",
      "  repa_loss.encoder.message_passing.W_h.weight\n",
      "  repa_loss.encoder.message_passing.W_o.weight\n",
      "  repa_loss.encoder.message_passing.W_o.bias\n"
     ]
    }
   ],
   "source": [
    "# REPA backward\n",
    "repa_model.zero_grad()\n",
    "torch.manual_seed(42)\n",
    "loss_repa, _ = repa_model(batch)\n",
    "loss_repa.backward()\n",
    "check_gradients(repa_model, \"REPA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Verify Optimizer Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline total params: 207\n",
      "Baseline trainable params: 207\n",
      "\n",
      "REPA total params: 215\n",
      "REPA trainable params: 211\n",
      "REPA frozen params: 4\n"
     ]
    }
   ],
   "source": [
    "# Count parameters that would be in optimizer\n",
    "baseline_params = list(baseline_model.parameters())\n",
    "repa_params = list(repa_model.parameters())\n",
    "\n",
    "baseline_trainable = [p for p in baseline_params if p.requires_grad]\n",
    "repa_trainable = [p for p in repa_params if p.requires_grad]\n",
    "\n",
    "print(f\"Baseline total params: {len(baseline_params)}\")\n",
    "print(f\"Baseline trainable params: {len(baseline_trainable)}\")\n",
    "print(\"\")\n",
    "print(f\"REPA total params: {len(repa_params)}\")\n",
    "print(f\"REPA trainable params: {len(repa_trainable)}\")\n",
    "print(f\"REPA frozen params: {len(repa_params) - len(repa_trainable)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters in optimizer:\n",
      "  Group 0: 215 params, lr=0.0001\n",
      "\n",
      "Note: Optimizer receives all params but only updates those with requires_grad=True\n"
     ]
    }
   ],
   "source": [
    "# Simulate optimizer step\n",
    "optimizer_repa = torch.optim.Adam(repa_model.parameters(), lr=1e-4)\n",
    "\n",
    "print(\"Parameters in optimizer:\")\n",
    "for i, pg in enumerate(optimizer_repa.param_groups):\n",
    "    print(f\"  Group {i}: {len(pg['params'])} params, lr={pg['lr']}\")\n",
    "\n",
    "# The optimizer.param_groups contains all params, but only trainable ones get updated\n",
    "print(\n",
    "    \"\\nNote: Optimizer receives all params but only updates those with requires_grad=True\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually trace REPA loss forward pass\n",
    "repa_model.zero_grad()\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Get the path and pred by stepping through manually\n",
    "batch_size = batch[\"padding_mask\"].shape[0]\n",
    "t = repa_model.time_distribution.sample((batch_size,))\n",
    "noise_batch = repa_model._sample_noise_like_batch(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8823, 0.9150, 0.3829, 0.9593])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interpolation path\n",
    "x_0_coords, x_t_coords, dx_t_coords = repa_model.coords_interpolant.create_path(\n",
    "    x_1=batch, t=t, x_0=noise_batch\n",
    ")\n",
    "x_0_atomics, x_t_atomics, dx_t_atomics = repa_model.atomics_interpolant.create_path(\n",
    "    x_1=batch, t=t, x_0=noise_batch\n",
    ")\n",
    "\n",
    "# Build x_t (noisy input to net)\n",
    "x_t = TensorDict(\n",
    "    {\n",
    "        \"coords\": x_t_coords,\n",
    "        \"atomics\": x_t_atomics,\n",
    "        \"padding_mask\": batch[\"padding_mask\"],\n",
    "    },\n",
    "    batch_size=[batch_size],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Verify x_1 vs x_t ===\n",
      "x_1 coords (clean): tensor([ 0.3589, -0.6821, -1.3073])\n",
      "x_t coords (noisy): tensor([ 0.4538, -0.7351, -1.2011])\n",
      "t value: 0.8823\n",
      "\n",
      "=== Hidden states ===\n",
      "hidden_states shape: torch.Size([4, 9, 128])\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Verify x_1 vs x_t ===\")\n",
    "print(f\"x_1 coords (clean): {batch['coords'][0, 0]}\")\n",
    "print(f\"x_t coords (noisy): {x_t['coords'][0, 0]}\")\n",
    "print(f\"t value: {t[0].item():.4f}\")\n",
    "\n",
    "# Get hidden states from net\n",
    "pred = repa_model._call_net(x_t, t, return_hidden_states=True)\n",
    "hidden_states = pred[\"hidden_states\"]\n",
    "padding_mask = pred[\"padding_mask\"]\n",
    "\n",
    "print(\"\\n=== Hidden states ===\")\n",
    "print(f\"hidden_states shape: {hidden_states.shape}\")  # [B, N, hidden_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 9, 128])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[\"hidden_states\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_repr shape: torch.Size([4, 9, 2048])\n"
     ]
    }
   ],
   "source": [
    "# Trace through REPA loss\n",
    "encoder = repa_model.repa_loss.encoder\n",
    "projector = repa_model.repa_loss.projector\n",
    "\n",
    "# Encoder sees CLEAN data (x_1)\n",
    "with torch.no_grad():\n",
    "    target_repr = encoder(\n",
    "        batch[\"coords\"],  # x_1 coords - CLEAN\n",
    "        batch[\"atomics\"],  # x_1 atomics - CLEAN\n",
    "        padding_mask,\n",
    "    )\n",
    "print(f\"target_repr shape: {target_repr.shape}\")  # [B, N, encoder_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "projected_repr shape: torch.Size([4, 9, 2048])\n",
      "\n",
      "=== Padding mask ===\n",
      "padding_mask: tensor([False, False, False, False, False, False, False, False, False])\n",
      "real_mask (inverted): tensor([True, True, True, True, True, True, True, True, True])\n",
      "Atoms used in loss: 9\n"
     ]
    }
   ],
   "source": [
    "# Projector sees hidden states (from x_t)\n",
    "projected_repr = projector(hidden_states)\n",
    "print(f\"projected_repr shape: {projected_repr.shape}\")\n",
    "\n",
    "# Check padding mask application\n",
    "real_mask = ~padding_mask\n",
    "print(\"\\n=== Padding mask ===\")\n",
    "print(f\"padding_mask: {padding_mask[0]}\")\n",
    "print(f\"real_mask (inverted): {real_mask[0]}\")\n",
    "print(f\"Atoms used in loss: {real_mask[0].sum().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Alignment ===\n",
      "cosine similarities (per atom): tensor([ 0.0015, -0.0042, -0.0148, -0.0158, -0.0304], grad_fn=<SliceBackward0>)\n",
      "mean cosine sim: -0.0025\n",
      "REPA loss (before lambda): 0.0025\n",
      "REPA loss (after lambda=0.5): 0.0013\n"
     ]
    }
   ],
   "source": [
    "# Compute cosine similarity on valid atoms only\n",
    "cos_sim = torch.nn.functional.cosine_similarity(\n",
    "    projected_repr[real_mask], target_repr[real_mask], dim=-1\n",
    ")\n",
    "print(\"\\n=== Alignment ===\")\n",
    "print(f\"cosine similarities (per atom): {cos_sim[:5]}\")  # First 5\n",
    "print(f\"mean cosine sim: {cos_sim.mean().item():.4f}\")\n",
    "print(f\"REPA loss (before lambda): {-cos_sim.mean().item():.4f}\")\n",
    "print(f\"REPA loss (after lambda=0.5): {-cos_sim.mean().item() * 0.5:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
