{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug REPA Training\n",
    "\n",
    "This notebook steps through forward/backward passes with real QM9 data to verify:\n",
    "- Correct parameter training (net + projector trainable, encoder frozen)\n",
    "- Gradient flow\n",
    "- REPA loss computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "# Change to project root\n",
    "os.chdir(\"/Users/shreyas/git/molecular-repa/src/tabasco\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load QM9 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabasco.data.lmdb_datamodule import LmdbDataModule\n",
    "\n",
    "dm = LmdbDataModule(\n",
    "    data_dir=\"data/processed_qm9_train.pt\",\n",
    "    val_data_dir=\"data/processed_qm9_val.pt\",\n",
    "    lmdb_dir=\"data/lmdb_qm9\",\n",
    "    batch_size=4,\n",
    "    num_workers=0,\n",
    ")\n",
    "dm.prepare_data()\n",
    "dm.setup()\n",
    "batch = next(iter(dm.train_dataloader()))\n",
    "\n",
    "print(f\"Batch keys: {batch.keys()}\")\n",
    "print(f\"coords shape: {batch['coords'].shape}\")\n",
    "print(f\"atomics shape: {batch['atomics'].shape}\")\n",
    "print(f\"padding_mask shape: {batch['padding_mask'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Baseline Model (no REPA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabasco.models.flow_model import FlowMatchingModel\n",
    "from tabasco.models.components.transformer_module import TransformerModule\n",
    "from tabasco.flow.interpolate import CenteredMetricInterpolant, DiscreteInterpolant\n",
    "\n",
    "atom_dim = batch[\"atomics\"].shape[-1]\n",
    "print(f\"atom_dim from data: {atom_dim}\")\n",
    "\n",
    "net = TransformerModule(\n",
    "    hidden_dim=64,\n",
    "    num_layers=2,\n",
    "    num_heads=4,\n",
    "    atom_dim=atom_dim,\n",
    "    spatial_dim=3,\n",
    "    implementation=\"pytorch\",\n",
    ")\n",
    "\n",
    "baseline_model = FlowMatchingModel(\n",
    "    net=net,\n",
    "    coords_interpolant=CenteredMetricInterpolant(\n",
    "        key=\"coords\", key_pad_mask=\"padding_mask\"\n",
    "    ),\n",
    "    atomics_interpolant=DiscreteInterpolant(key=\"atomics\", key_pad_mask=\"padding_mask\"),\n",
    "    repa_loss=None,\n",
    ")\n",
    "\n",
    "print(\"Baseline model created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create REPA Model (with ChemProp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabasco.models.components.encoders import ChemPropEncoder, Projector\n",
    "from tabasco.models.components.losses import REPALoss\n",
    "\n",
    "net_repa = TransformerModule(\n",
    "    hidden_dim=64,\n",
    "    num_layers=2,\n",
    "    num_heads=4,\n",
    "    atom_dim=atom_dim,\n",
    "    spatial_dim=3,\n",
    "    implementation=\"pytorch\",\n",
    ")\n",
    "\n",
    "encoder = ChemPropEncoder(pretrained=\"chemeleon\")\n",
    "print(f\"Encoder dim: {encoder.encoder_dim}\")\n",
    "\n",
    "projector = Projector(hidden_dim=64, encoder_dim=encoder.encoder_dim)\n",
    "repa_loss = REPALoss(encoder=encoder, projector=projector, lambda_repa=0.5)\n",
    "\n",
    "repa_model = FlowMatchingModel(\n",
    "    net=net_repa,\n",
    "    coords_interpolant=CenteredMetricInterpolant(\n",
    "        key=\"coords\", key_pad_mask=\"padding_mask\"\n",
    "    ),\n",
    "    atomics_interpolant=DiscreteInterpolant(key=\"atomics\", key_pad_mask=\"padding_mask\"),\n",
    "    repa_loss=repa_loss,\n",
    ")\n",
    "\n",
    "print(\"REPA model created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Verify Parameter States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_params(model, name):\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    trainable = 0\n",
    "    frozen = 0\n",
    "    for n, p in model.named_parameters():\n",
    "        status = \"TRAINABLE\" if p.requires_grad else \"FROZEN\"\n",
    "        print(f\"{n}: {status}, shape={tuple(p.shape)}\")\n",
    "        if p.requires_grad:\n",
    "            trainable += p.numel()\n",
    "        else:\n",
    "            frozen += p.numel()\n",
    "    print(f\"\\nTotal trainable: {trainable:,}\")\n",
    "    print(f\"Total frozen: {frozen:,}\")\n",
    "\n",
    "\n",
    "check_params(baseline_model, \"Baseline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_params(repa_model, \"REPA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Forward Pass Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline forward\n",
    "with torch.no_grad():\n",
    "    loss_baseline, stats_baseline = baseline_model(batch, compute_stats=True)\n",
    "\n",
    "print(f\"Baseline loss: {loss_baseline.item():.4f}\")\n",
    "print(f\"Baseline stats: {stats_baseline}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REPA forward\n",
    "with torch.no_grad():\n",
    "    loss_repa, stats_repa = repa_model(batch, compute_stats=True)\n",
    "\n",
    "print(f\"REPA loss: {loss_repa.item():.4f}\")\n",
    "print(f\"REPA stats: {stats_repa}\")\n",
    "\n",
    "# Check REPA-specific stats\n",
    "if \"repa_loss\" in stats_repa:\n",
    "    print(f\"\\nREPA loss component: {stats_repa['repa_loss']:.4f}\")\n",
    "if \"repa_alignment\" in stats_repa:\n",
    "    print(f\"REPA alignment: {stats_repa['repa_alignment']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Backward Pass - Check Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_gradients(model, name):\n",
    "    print(f\"\\n=== {name} Gradients ===\")\n",
    "    has_grad = []\n",
    "    no_grad = []\n",
    "    for n, p in model.named_parameters():\n",
    "        if p.grad is not None:\n",
    "            grad_norm = p.grad.abs().sum().item()\n",
    "            has_grad.append((n, grad_norm))\n",
    "        else:\n",
    "            no_grad.append(n)\n",
    "\n",
    "    print(\"\\nParameters WITH gradients:\")\n",
    "    for n, g in has_grad:\n",
    "        print(f\"  {n}: grad_norm={g:.6f}\")\n",
    "\n",
    "    print(f\"\\nParameters WITHOUT gradients ({len(no_grad)}):\")\n",
    "    for n in no_grad:\n",
    "        print(f\"  {n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline backward\n",
    "baseline_model.zero_grad()\n",
    "loss_baseline, _ = baseline_model(batch)\n",
    "loss_baseline.backward()\n",
    "check_gradients(baseline_model, \"Baseline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REPA backward\n",
    "repa_model.zero_grad()\n",
    "loss_repa, _ = repa_model(batch)\n",
    "loss_repa.backward()\n",
    "check_gradients(repa_model, \"REPA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Verify Optimizer Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count parameters that would be in optimizer\n",
    "baseline_params = list(baseline_model.parameters())\n",
    "repa_params = list(repa_model.parameters())\n",
    "\n",
    "baseline_trainable = [p for p in baseline_params if p.requires_grad]\n",
    "repa_trainable = [p for p in repa_params if p.requires_grad]\n",
    "\n",
    "print(f\"Baseline total params: {len(baseline_params)}\")\n",
    "print(f\"Baseline trainable params: {len(baseline_trainable)}\")\n",
    "print(\"\")\n",
    "print(f\"REPA total params: {len(repa_params)}\")\n",
    "print(f\"REPA trainable params: {len(repa_trainable)}\")\n",
    "print(f\"REPA frozen params: {len(repa_params) - len(repa_trainable)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate optimizer step\n",
    "optimizer_repa = torch.optim.Adam(repa_model.parameters(), lr=1e-4)\n",
    "\n",
    "print(\"Parameters in optimizer:\")\n",
    "for i, pg in enumerate(optimizer_repa.param_groups):\n",
    "    print(f\"  Group {i}: {len(pg['params'])} params, lr={pg['lr']}\")\n",
    "\n",
    "# The optimizer.param_groups contains all params, but only trainable ones get updated\n",
    "print(\n",
    "    \"\\nNote: Optimizer receives all params but only updates those with requires_grad=True\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Verification Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"VERIFICATION CHECKLIST\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check encoder params are frozen\n",
    "encoder_frozen = all(\n",
    "    not p.requires_grad for p in repa_model.repa_loss.encoder.parameters()\n",
    ")\n",
    "print(f\"[{'✓' if encoder_frozen else '✗'}] Encoder params have requires_grad=False\")\n",
    "\n",
    "# Check encoder has no gradients\n",
    "encoder_no_grad = all(p.grad is None for p in repa_model.repa_loss.encoder.parameters())\n",
    "print(\n",
    "    f\"[{'✓' if encoder_no_grad else '✗'}] Encoder params have grad=None after backward\"\n",
    ")\n",
    "\n",
    "# Check projector has gradients\n",
    "projector_has_grad = any(\n",
    "    p.grad is not None and p.grad.abs().sum() > 0\n",
    "    for p in repa_model.repa_loss.projector.parameters()\n",
    ")\n",
    "print(\n",
    "    f\"[{'✓' if projector_has_grad else '✗'}] Projector params have non-zero gradients\"\n",
    ")\n",
    "\n",
    "# Check net has gradients\n",
    "net_has_grad = any(\n",
    "    p.grad is not None and p.grad.abs().sum() > 0 for p in repa_model.net.parameters()\n",
    ")\n",
    "print(f\"[{'✓' if net_has_grad else '✗'}] Net params have non-zero gradients\")\n",
    "\n",
    "# Check REPA loss is non-zero\n",
    "repa_loss_nonzero = \"repa_loss\" in stats_repa and stats_repa[\"repa_loss\"] != 0\n",
    "print(f\"[{'✓' if repa_loss_nonzero else '✗'}] REPA loss is non-zero\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
