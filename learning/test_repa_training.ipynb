{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test REPA Training with TABASCO\n",
    "\n",
    "This notebook tests the REPA integration with a small dataset using TABASCO's actual training infrastructure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create Tiny Dataset\n",
    "\n",
    "First, let's create a small dataset of simple molecules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset\n",
    "%run create_tiny_dataset.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup Imports and Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tensordict import TensorDict\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# TABASCO imports\n",
    "from tabasco.models.components.transformer_module import TransformerModule\n",
    "from tabasco.models.components.encoders import DummyEncoder, Projector\n",
    "from tabasco.models.components.losses import REPALoss\n",
    "from tabasco.models.flow_model import FlowMatchingModel\n",
    "from tabasco.flow.interpolate import CoordsInterpolant, AtomicsInterpolant\n",
    "\n",
    "# Setup device\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyMoleculeDataset(Dataset):\n",
    "    \"\"\"Simple dataset wrapper for tiny molecule dataset.\"\"\"\n",
    "    \n",
    "    def __init__(self, data_path: str, max_num_atoms: int = 30):\n",
    "        self.batches = torch.load(data_path)\n",
    "        self.max_num_atoms = max_num_atoms\n",
    "        print(f\"Loaded {len(self.batches)} molecules\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.batches)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        batch = self.batches[idx]\n",
    "        \n",
    "        # Pad to max_num_atoms\n",
    "        num_atoms = batch[\"coords\"].shape[0]\n",
    "        \n",
    "        if num_atoms > self.max_num_atoms:\n",
    "            # Truncate if too large\n",
    "            coords = batch[\"coords\"][:self.max_num_atoms]\n",
    "            atomics = batch[\"atomics\"][:self.max_num_atoms]\n",
    "            padding_mask = torch.zeros(self.max_num_atoms, dtype=torch.bool)\n",
    "        else:\n",
    "            # Pad if too small\n",
    "            pad_size = self.max_num_atoms - num_atoms\n",
    "            coords = torch.cat([\n",
    "                batch[\"coords\"],\n",
    "                torch.zeros(pad_size, 3)\n",
    "            ])\n",
    "            atomics = torch.cat([\n",
    "                batch[\"atomics\"],\n",
    "                torch.zeros(pad_size, batch[\"atomics\"].shape[1])\n",
    "            ])\n",
    "            padding_mask = torch.cat([\n",
    "                torch.zeros(num_atoms, dtype=torch.bool),\n",
    "                torch.ones(pad_size, dtype=torch.bool)\n",
    "            ])\n",
    "        \n",
    "        return TensorDict({\n",
    "            \"coords\": coords,\n",
    "            \"atomics\": atomics,\n",
    "            \"padding_mask\": padding_mask\n",
    "        }, batch_size=[])\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = TinyMoleculeDataset(\"tiny_dataset.pt\", max_num_atoms=30)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=lambda x: torch.stack(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Model with REPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparameters\n",
    "spatial_dim = 3\n",
    "atom_dim = 10  # Assuming 10 atom types (adjust based on dataset)\n",
    "hidden_dim = 128\n",
    "encoder_dim = 256\n",
    "num_heads = 4\n",
    "num_layers = 3\n",
    "\n",
    "# Create transformer\n",
    "transformer = TransformerModule(\n",
    "    spatial_dim=spatial_dim,\n",
    "    atom_dim=atom_dim,\n",
    "    num_heads=num_heads,\n",
    "    num_layers=num_layers,\n",
    "    hidden_dim=hidden_dim,\n",
    "    implementation=\"pytorch\"\n",
    ").to(device)\n",
    "\n",
    "# Create REPA components\n",
    "encoder = DummyEncoder(\n",
    "    input_dim=spatial_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    encoder_dim=encoder_dim\n",
    ").to(device)\n",
    "\n",
    "projector = Projector(\n",
    "    hidden_dim=hidden_dim,\n",
    "    encoder_dim=encoder_dim,\n",
    "    num_layers=2\n",
    ").to(device)\n",
    "\n",
    "repa_loss = REPALoss(\n",
    "    encoder=encoder,\n",
    "    projector=projector,\n",
    "    lambda_repa=0.5,\n",
    "    time_weighting=False\n",
    ")\n",
    "\n",
    "# Create interpolants\n",
    "coords_interpolant = CoordsInterpolant()\n",
    "atomics_interpolant = AtomicsInterpolant()\n",
    "\n",
    "# Create flow matching model WITH REPA\n",
    "model_with_repa = FlowMatchingModel(\n",
    "    net=transformer,\n",
    "    coords_interpolant=coords_interpolant,\n",
    "    atomics_interpolant=atomics_interpolant,\n",
    "    repa_loss=repa_loss,  # Enable REPA!\n",
    "    time_distribution=\"uniform\"\n",
    ").to(device)\n",
    "\n",
    "print(\"✓ Model with REPA created\")\n",
    "print(f\"  Transformer parameters: {sum(p.numel() for p in transformer.parameters()):,}\")\n",
    "print(f\"  Encoder parameters (frozen): {sum(p.numel() for p in encoder.parameters()):,}\")\n",
    "print(f\"  Projector parameters (trainable): {sum(p.numel() for p in projector.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create Baseline Model (without REPA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a second transformer for baseline comparison\n",
    "transformer_baseline = TransformerModule(\n",
    "    spatial_dim=spatial_dim,\n",
    "    atom_dim=atom_dim,\n",
    "    num_heads=num_heads,\n",
    "    num_layers=num_layers,\n",
    "    hidden_dim=hidden_dim,\n",
    "    implementation=\"pytorch\"\n",
    ").to(device)\n",
    "\n",
    "# Create flow matching model WITHOUT REPA\n",
    "model_baseline = FlowMatchingModel(\n",
    "    net=transformer_baseline,\n",
    "    coords_interpolant=CoordsInterpolant(),\n",
    "    atomics_interpolant=AtomicsInterpolant(),\n",
    "    repa_loss=None,  # No REPA\n",
    "    time_distribution=\"uniform\"\n",
    ").to(device)\n",
    "\n",
    "print(\"✓ Baseline model (no REPA) created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Setup Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer for REPA model (transformer + projector)\n",
    "optimizer_repa = torch.optim.Adam(\n",
    "    list(transformer.parameters()) + list(projector.parameters()),\n",
    "    lr=1e-4\n",
    ")\n",
    "\n",
    "# Optimizer for baseline model (transformer only)\n",
    "optimizer_baseline = torch.optim.Adam(\n",
    "    transformer_baseline.parameters(),\n",
    "    lr=1e-4\n",
    ")\n",
    "\n",
    "print(\"✓ Optimizers created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, device, model_name=\"Model\"):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    epoch_losses = []\n",
    "    epoch_stats = []\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc=f\"{model_name}\")\n",
    "    for batch in pbar:\n",
    "        # Move batch to device\n",
    "        batch = batch.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss, stats = model(batch, compute_stats=True)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track metrics\n",
    "        epoch_losses.append(loss.item())\n",
    "        epoch_stats.append(stats)\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({\n",
    "            'loss': f\"{loss.item():.4f}\",\n",
    "            'repa_loss': f\"{stats.get('repa_loss', 0):.4f}\",\n",
    "            'repa_align': f\"{stats.get('repa_alignment', 0):.4f}\"\n",
    "        })\n",
    "    \n",
    "    return epoch_losses, epoch_stats\n",
    "\n",
    "# Train both models\n",
    "num_epochs = 10\n",
    "\n",
    "# Storage for metrics\n",
    "losses_repa = []\n",
    "losses_baseline = []\n",
    "repa_losses = []\n",
    "repa_alignments = []\n",
    "\n",
    "print(f\"\\nTraining for {num_epochs} epochs...\\n\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Train REPA model\n",
    "    epoch_losses_repa, epoch_stats_repa = train_epoch(\n",
    "        model_with_repa, dataloader, optimizer_repa, device, \"REPA Model\"\n",
    "    )\n",
    "    losses_repa.extend(epoch_losses_repa)\n",
    "    repa_losses.extend([s.get('repa_loss', 0) for s in epoch_stats_repa])\n",
    "    repa_alignments.extend([s.get('repa_alignment', 0) for s in epoch_stats_repa])\n",
    "    \n",
    "    # Train baseline model\n",
    "    epoch_losses_baseline, _ = train_epoch(\n",
    "        model_baseline, dataloader, optimizer_baseline, device, \"Baseline Model\"\n",
    "    )\n",
    "    losses_baseline.extend(epoch_losses_baseline)\n",
    "    \n",
    "    # Print epoch summary\n",
    "    print(f\"\\nEpoch {epoch + 1} Summary:\")\n",
    "    print(f\"  REPA Model - Avg Loss: {sum(epoch_losses_repa)/len(epoch_losses_repa):.4f}\")\n",
    "    print(f\"  Baseline Model - Avg Loss: {sum(epoch_losses_baseline)/len(epoch_losses_baseline):.4f}\")\n",
    "\n",
    "print(\"\\n✓ Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Plot 1: Total Loss Comparison\n",
    "axes[0, 0].plot(losses_repa, label='REPA Model', alpha=0.7)\n",
    "axes[0, 0].plot(losses_baseline, label='Baseline Model', alpha=0.7)\n",
    "axes[0, 0].set_title('Total Loss Comparison')\n",
    "axes[0, 0].set_xlabel('Training Step')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: REPA Loss\n",
    "axes[0, 1].plot(repa_losses, color='red', alpha=0.7)\n",
    "axes[0, 1].set_title('REPA Alignment Loss')\n",
    "axes[0, 1].set_xlabel('Training Step')\n",
    "axes[0, 1].set_ylabel('REPA Loss')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: REPA Alignment (Cosine Similarity)\n",
    "axes[1, 0].plot(repa_alignments, color='green', alpha=0.7)\n",
    "axes[1, 0].set_title('REPA Alignment (Cosine Similarity)')\n",
    "axes[1, 0].set_xlabel('Training Step')\n",
    "axes[1, 0].set_ylabel('Alignment')\n",
    "axes[1, 0].axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Smoothed Loss Comparison\n",
    "window = 5\n",
    "def smooth(data, window_size):\n",
    "    return [sum(data[max(0, i-window_size):i+1]) / min(i+1, window_size) \n",
    "            for i in range(len(data))]\n",
    "\n",
    "axes[1, 1].plot(smooth(losses_repa, window), label='REPA Model (smoothed)', linewidth=2)\n",
    "axes[1, 1].plot(smooth(losses_baseline, window), label='Baseline Model (smoothed)', linewidth=2)\n",
    "axes[1, 1].set_title(f'Loss Comparison (smoothed, window={window})')\n",
    "axes[1, 1].set_xlabel('Training Step')\n",
    "axes[1, 1].set_ylabel('Loss')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('repa_training_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Results plotted and saved to repa_training_results.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Analysis & Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Final losses\n",
    "final_loss_repa = sum(losses_repa[-10:]) / 10\n",
    "final_loss_baseline = sum(losses_baseline[-10:]) / 10\n",
    "\n",
    "print(f\"\\nFinal Loss (avg of last 10 steps):\")\n",
    "print(f\"  REPA Model:     {final_loss_repa:.4f}\")\n",
    "print(f\"  Baseline Model: {final_loss_baseline:.4f}\")\n",
    "print(f\"  Difference:     {final_loss_baseline - final_loss_repa:.4f}\")\n",
    "\n",
    "if final_loss_repa < final_loss_baseline:\n",
    "    print(f\"\\n✓ REPA model achieves {((final_loss_baseline - final_loss_repa)/final_loss_baseline * 100):.1f}% lower loss!\")\n",
    "else:\n",
    "    print(f\"\\n⚠ Baseline model achieves {((final_loss_repa - final_loss_baseline)/final_loss_repa * 100):.1f}% lower loss\")\n",
    "\n",
    "# REPA metrics\n",
    "final_repa_loss = sum(repa_losses[-10:]) / 10\n",
    "final_alignment = sum(repa_alignments[-10:]) / 10\n",
    "\n",
    "print(f\"\\nREPA Metrics (avg of last 10 steps):\")\n",
    "print(f\"  REPA Loss:       {final_repa_loss:.4f}\")\n",
    "print(f\"  Alignment Score: {final_alignment:.4f}\")\n",
    "\n",
    "# Check if alignment is improving\n",
    "early_alignment = sum(repa_alignments[:10]) / 10\n",
    "alignment_improvement = final_alignment - early_alignment\n",
    "\n",
    "print(f\"\\nAlignment Improvement:\")\n",
    "print(f\"  Early alignment:  {early_alignment:.4f}\")\n",
    "print(f\"  Final alignment:  {final_alignment:.4f}\")\n",
    "print(f\"  Improvement:      {alignment_improvement:+.4f}\")\n",
    "\n",
    "if alignment_improvement > 0:\n",
    "    print(f\"\\n✓ Alignment is improving over training! (+{alignment_improvement:.4f})\")\n",
    "else:\n",
    "    print(f\"\\n⚠ Alignment decreased over training ({alignment_improvement:.4f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✓ REPA INTEGRATION TEST COMPLETE\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Test Generation (Optional)\n",
    "\n",
    "Let's test if the trained model can generate molecules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a few molecules with REPA model\n",
    "model_with_repa.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Set data stats for sampling\n",
    "    model_with_repa.set_data_stats({\n",
    "        'max_num_atoms': 30,\n",
    "        'spatial_dim': 3,\n",
    "        'atom_dim': atom_dim,\n",
    "        'num_atoms_histogram': {i: 1 for i in range(5, 25)}  # Uniform distribution\n",
    "    })\n",
    "    \n",
    "    # Sample\n",
    "    generated = model_with_repa.sample(\n",
    "        batch_size=3,\n",
    "        num_steps=50\n",
    "    )\n",
    "    \n",
    "    print(\"✓ Generated 3 molecules\")\n",
    "    print(f\"  Coords shape: {generated['coords'].shape}\")\n",
    "    print(f\"  Atomics shape: {generated['atomics'].shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
