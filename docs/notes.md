# 20251221

## Diffusion

General idea
1. Start with a dataset
2. At each time step $t$, add Gaussian noise -- as time goes to infinity this will yield a fully Gaussian noise distribution
3. Then learn a NN to revert one of those noise-inducing steps


## Flow matching

https://scontent-lhr8-1.xx.fbcdn.net/v/t39.2365-6/469963300_2320719918292896_7950025307614718519_n.pdf?_nc_cat=108&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=TKEh9-YHJb4Q7kNvwG0Zxfx&_nc_oc=AdmYWrXsXw4J5KkCV3-cafWYG3vYgtEIad0ThotckIyC4GB9bEtMB_Qq5iCEQewsWdc&_nc_zt=14&_nc_ht=scontent-lhr8-1.xx&_nc_gid=zAyIXhQKL6-4zj2izSnQjA&oh=00_Afk5dKkC3W3nYunaQQ8QwQOudg9Sd7o18ecImyfd6SHWbQ&oe=694E3C02

See notebook pages 49-55

https://www.youtube.com/watch?v=7NNxK3CqaDk

Idea: instead of doing the iterative noise adding/removal process in diffusion, why not just work on the distributions directly? Can we just morph the original noise distribution into the data distribution? Without explicity specifying the process as diffusion does

"How do you learn a flow from one distribution to another while only having samples from the second?"

Paper suggests that solving this problem for individual/single paths and aggregating them is enough to characterise the entire distribution


Flow matching is a more general version of diffusion

This paper is mostly about 'how do you construct distributions from samples'
1. Construct a target prob dist using the samples we have -- for each sample, create a Gaussian with mean = sample, then a Gaussian mixture model across all individual Gaussians?
2. 

https://www.youtube.com/watch?v=7cMzfkWFWhI

This video deals with samples. not probability distributions

- Assume that the underlying data distribution that data x1 comes from is $p_1(x_1)$
- Want to learn a NN that helps you approximate and sample from this unknown
- Idea: Start from some known $p_0(x_0)$ and convert it into  $p_1(x_1)$
  - Gaussian noise -> data distribution, following some path
  - Want to learn some NN that learns this path
- Easiest way to construct the path is linear interpolation $x_t = tx_1 + (1-t)x_0$
- This is the optimal transport (OT) displacement map -- the 'straightest' possible path between $x_0$ and $x_1$ i.e. between noise and data
- Want to know dxt/dt because this tells you how much xt changes and therefore how to move closer to data
- $\frac{d}{dt}x_t = x_1 - x_0$  = vector that points towards x1 from x0
- So you want to learn this!

- Different ways to approach learning $p_t(x_t)$:
  - Score matching: use $\nabla_{x_t} \log p_t(x_t)$
  - DDPM: start from $ -\log p_t(x_t)$
  - Flow matching: $p_t(x_t)$ is generated by a ‘flow’
    - $x_t = \psi_t(x)$
      - so $x_t$ can be re-created from some original point $x$ using the flow which is time-indexed
    - $\frac{d}{dt}\psi_t(x) = u_t(\psi_t(x))$ 
    - this is the same as saying $\frac{d}{dt}x_t = u_t(x_t)$
    - $u_t$ is a vector field -- points in the direction to move towards the data distribution
      - Note that the vector field is time-dependent i.e. it changes with $t$ as well
      - However the optimal transport parameterisation skips this by just assuming time-dependency because the vector field is constant $(x_1 - x_0)$ for all $t$
    - flow matching tries to approximate this flow vector field $u_t$ using a learned NN $v_t$, so the outputs of both functions looks the same given $x_t$
- Objective?
  - want to learn a NN that learns an approximation of the derivative of the flow with respect to time
  - During training:
    - sample random noise $x_0$
    - sample data $x_1$ from dataset
    - sample $t$ between 0 and 1
    - calculate $x_t$ by linearly interpolating between $x_0$ and $x_1$ with the flow = $t x_1 + (1-t) x_0$
    - this is input to NN
    - NN tries to predict vector from $x_0$ to $x_t$
    - input = ($x_t$, $t$) -> output = $v_t(x_t)$
    - minimize $|| v_t(x_t) - (x_1 - x_0) ||^2$
    - this is the *conditional flow matching objective*
      - in reality, we don't know $u_t$ for the entire distribution
      - but we do know the vector field for a single pair of $(x_0, x_1)$
      - The key idea is that by sampling many pairs, the network attmepts to learn the 'average' vector field that transforms the whole noise distribution into the whole data distribution
- Sampling?
  - Numerical integration?
  - Start with $x_0$ = random noise
  - set number of steps e.g. 1000
  ```
  x_t = random(n, d)
  for t in range(steps):
    x_t = x_t + (1/steps) * model(x_t, t)
  ```
- Why doesn't flow matching collapse to a single solution?
  - We always use $(x_1 - x_0)$ as the target, so why doesn't the model just always output this constant?
  - The answer is that the model never sees $x_1$ or $x_0$ -- it only sees their mixture $x_t$. But $x_t$ could lie on a line between any two points $x_0$ and $x_1$!
  - So the model never knows 'Which $x_1$ should I solve for?'
- Why not just use $x_1$ as the target instead of $(x_1 - x_0)$?
  - $x_1$ as target would turn this into an auto-encoder or de-noiser?
  - Given we start from random noise, and there are many possible $x_1$s, using $x_1$ as a target would cause the model to always predict the mean $x_1$ as the output --> model collapse?
  - The flow matching + REPA project kind of does this?
    - flow matching = how to move (target is $(x_1 - x_0)$)
    - REPA = what are you moving towards (target is $x_1$)
- REPA and flow matching
  - The Flow Matching view: The model is trying to find the "average" vector field vt​(xt​,t). To do this well, it needs to understand the "meaning" of xt​.
  - The REPA Intervention: You are adding a loss term that says: "While you are calculating the velocity at xt​, your internal hidden layers should produce a feature vector that is similar to what MACE would produce for a molecule at that same stage."
  - Addressing the "Noisy Molecule" Problem
    - You asked earlier if the vector field is time-independent. In your thesis, t is actually your best friend.
    - At t=1, xt​ is a clean molecule. MACE or Chemprop will give you a very clear, meaningful embedding.
    - At t=0, xt​ is Gaussian noise. A molecular encoder like MACE might give you "garbage" embeddings because it wasn't trained on random noise.
    - Your Challenge: You will need to see if aligning representations at t≈0 (high noise) is actually helpful, or if REPA only provides a useful signal when the "molecule" starts to take shape (t>0.5).
    ```# Training Loop
    for x1 in dataloader: # x1 is a real molecule (Batch, D)
        x0 = torch.randn_like(x1) # x0 is noise (Batch, D) - SAME DIMENSION
        t = torch.rand(1) # random time
        
        # 1. Create the point on the path (Linear Interpolation)
        xt = (1 - t) * x0 + t * x1
        
        # 2. Get the model's predicted velocity AND its internal features
        # v_pred: (Batch, D), features: (Batch, Hidden_Dim)
        v_pred, features = model(xt, t) 
        
        # 3. Flow Matching Loss: Follow the straight line
        target_v = x1 - x0
        loss_fm = F.mse_loss(v_pred, target_v)
        
        # 4. REPA Loss: Align with pre-trained encoder (e.g., MACE)
        with torch.no_grad():
            target_features = pretrained_encoder(x1) # Align with clean data representation
        loss_repa = F.mse_loss(features, target_features)
        
        # Total Loss
        total_loss = loss_fm + lambda_repa * loss_repa
        total_loss.backward()
    ```
  - Flow Matching works best when the data x1​ shares an underlying structure. In molecules, that structure is "Physics and Valency." As long as that structure exists, the model can handle immense diversity (millions of different molecules) because the "rules" for moving atoms into valid positions are consistent across the whole set.